<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Evalis AI ‚Äì Hands-Free Interview</title>

<style>
:root { 
  --primary:#6366f1; 
  --bg:#020617;
  --success:#10b981;
  --listening:#f59e0b;
}

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

body{
  background: linear-gradient(135deg, #020617 0%, #0f172a 100%);
  color: #fff;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
  display: flex;
  justify-content: center;
  align-items: center;
  min-height: 100vh;
  padding: 20px;
}

.container {
  width: 100%;
  max-width: 600px;
}

.card{
  background: rgba(255,255,255,.06);
  backdrop-filter: blur(10px);
  border-radius: 24px;
  padding: 40px;
  text-align: center;
  border: 1px solid rgba(255,255,255,.08);
  box-shadow: 0 20px 60px rgba(0,0,0,.3);
}

h1 {
  font-size: 28px;
  margin-bottom: 12px;
  background: linear-gradient(135deg, #6366f1, #8b5cf6);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}

.subtitle {
  color: #94a3b8;
  margin-bottom: 32px;
  font-size: 14px;
}

/* Voice Visualizer */
.voice-container {
  position: relative;
  width: 180px;
  height: 180px;
  margin: 0 auto 32px;
}

.voice-circle {
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  width: 140px;
  height: 140px;
  border-radius: 50%;
  background: linear-gradient(135deg, var(--primary), #8b5cf6);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 48px;
  transition: all 0.3s ease;
  box-shadow: 0 0 0 0 rgba(99, 102, 241, 0.4);
}

.voice-circle.listening {
  animation: pulse 2s infinite;
  background: linear-gradient(135deg, var(--listening), #f97316);
}

.voice-circle.speaking {
  background: linear-gradient(135deg, var(--success), #059669);
  animation: speaking 1s infinite;
}

@keyframes pulse {
  0%, 100% {
    box-shadow: 0 0 0 0 rgba(245, 158, 11, 0.7);
  }
  50% {
    box-shadow: 0 0 0 20px rgba(245, 158, 11, 0);
  }
}

@keyframes speaking {
  0%, 100% { transform: translate(-50%, -50%) scale(1); }
  50% { transform: translate(-50%, -50%) scale(1.05); }
}

.wave {
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  width: 140px;
  height: 140px;
  border-radius: 50%;
  border: 2px solid var(--listening);
  opacity: 0;
}

.wave.active {
  animation: wave 2s infinite;
}

@keyframes wave {
  0% {
    width: 140px;
    height: 140px;
    opacity: 0.6;
  }
  100% {
    width: 200px;
    height: 200px;
    opacity: 0;
  }
}

.status-text {
  font-size: 18px;
  font-weight: 600;
  margin-bottom: 8px;
  min-height: 28px;
}

.status-subtext {
  color: #94a3b8;
  font-size: 14px;
  margin-bottom: 24px;
  min-height: 20px;
}

.btn{
  width: 100%;
  padding: 18px;
  margin-top: 12px;
  border: none;
  border-radius: 14px;
  font-size: 16px;
  font-weight: 700;
  background: linear-gradient(135deg, var(--primary), #8b5cf6);
  color: white;
  cursor: pointer;
  transition: all 0.3s ease;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.btn:hover {
  transform: translateY(-2px);
  box-shadow: 0 10px 30px rgba(99, 102, 241, 0.4);
}

.btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
  transform: none;
}

.btn.stop {
  background: linear-gradient(135deg, #ef4444, #dc2626);
}

.btn.stop:hover {
  box-shadow: 0 10px 30px rgba(239, 68, 68, 0.4);
}

/* Conversation Log */
.conversation {
  margin-top: 32px;
  max-height: 300px;
  overflow-y: auto;
  text-align: left;
  padding: 20px;
  background: rgba(0,0,0,.2);
  border-radius: 16px;
}

.conversation::-webkit-scrollbar {
  width: 6px;
}

.conversation::-webkit-scrollbar-thumb {
  background: rgba(99, 102, 241, 0.5);
  border-radius: 3px;
}

.message {
  margin-bottom: 16px;
  padding: 12px 16px;
  border-radius: 12px;
  animation: slideIn 0.3s ease;
}

@keyframes slideIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.message.user {
  background: rgba(99, 102, 241, 0.2);
  border-left: 3px solid var(--primary);
}

.message.ai {
  background: rgba(16, 185, 129, 0.2);
  border-left: 3px solid var(--success);
}

.message .label {
  font-size: 12px;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  margin-bottom: 6px;
  opacity: 0.7;
}

.message .text {
  font-size: 14px;
  line-height: 1.5;
}

.settings {
  margin-top: 20px;
  padding-top: 20px;
  border-top: 1px solid rgba(255,255,255,.1);
  display: flex;
  gap: 16px;
  flex-wrap: wrap;
  justify-content: center;
}

.setting {
  display: flex;
  align-items: center;
  gap: 8px;
  font-size: 13px;
  color: #94a3b8;
}

.setting input[type="range"] {
  width: 100px;
}
</style>
</head>

<body>

<div class="container">
  <div class="card">
    <h1>üéôÔ∏è Evalis AI Interview</h1>
    <p class="subtitle">Hands-Free Voice Experience</p>

    <!-- Voice Visualizer -->
    <div class="voice-container">
      <div class="wave"></div>
      <div class="wave" style="animation-delay: 0.5s;"></div>
      <div class="voice-circle" id="voiceCircle">
        üé§
      </div>
    </div>

    <div class="status-text" id="statusText">Ready to Start</div>
    <div class="status-subtext" id="statusSubtext">Click below to begin your interview</div>

    <button id="startBtn" class="btn">üöÄ Start Hands-Free Interview</button>
    <button id="stopBtn" class="btn stop" style="display:none;">‚èπÔ∏è End Interview</button>

    <!-- Conversation Log -->
    <div class="conversation" id="conversation" style="display:none;"></div>

    <!-- Settings -->
    <div class="settings">
      <div class="setting">
        <span>Silence Detection:</span>
        <input type="range" id="silenceTime" min="1000" max="4000" value="2000" step="500">
        <span id="silenceValue">2s</span>
      </div>
    </div>
  </div>
</div>

<script>
/* ================= CONFIG ================= */
const API = "https://evalis-ai.simpaticohrconsultancy.workers.dev";

/* ================= STATE ================= */
let recognition;
let audioUnlocked = false;
let history = [];
let isListening = false;
let isProcessing = false;
let isSpeaking = false;
let interviewActive = false;
let silenceTimer;
let silenceTimeout = 2000; // ms of silence before auto-send

/* ================= UI ELEMENTS ================= */
const voiceCircle = document.getElementById("voiceCircle");
const statusText = document.getElementById("statusText");
const statusSubtext = document.getElementById("statusSubtext");
const conversation = document.getElementById("conversation");
const startBtn = document.getElementById("startBtn");
const stopBtn = document.getElementById("stopBtn");
const waves = document.querySelectorAll(".wave");
const silenceSlider = document.getElementById("silenceTime");
const silenceValue = document.getElementById("silenceValue");

/* ================= SETTINGS ================= */
silenceSlider.oninput = (e) => {
  silenceTimeout = parseInt(e.target.value);
  silenceValue.textContent = (silenceTimeout / 1000) + "s";
};

/* ================= UI UPDATES ================= */
function setStatus(main, sub, state) {
  statusText.textContent = main;
  statusSubtext.textContent = sub;
  
  voiceCircle.className = "voice-circle";
  waves.forEach(w => w.classList.remove("active"));
  
  if (state === "listening") {
    voiceCircle.classList.add("listening");
    voiceCircle.textContent = "üëÇ";
    waves.forEach(w => w.classList.add("active"));
  } else if (state === "speaking") {
    voiceCircle.classList.add("speaking");
    voiceCircle.textContent = "üó£Ô∏è";
  } else if (state === "processing") {
    voiceCircle.textContent = "ü§î";
  } else {
    voiceCircle.textContent = "üé§";
  }
}

function addMessage(type, text) {
  conversation.style.display = "block";
  const msg = document.createElement("div");
  msg.className = `message ${type}`;
  msg.innerHTML = `
    <div class="label">${type === "user" ? "You" : "AI Interviewer"}</div>
    <div class="text">${text}</div>
  `;
  conversation.insertBefore(msg, conversation.firstChild);
}

/* ================= AUDIO UNLOCK ================= */
function unlockAudio() {
  if (audioUnlocked) return;
  audioUnlocked = true;
  const u = new SpeechSynthesisUtterance(" ");
  speechSynthesis.speak(u);
}

/* ================= AI SPEAK ================= */
function speakAI(text) {
  return new Promise((resolve) => {
    unlockAudio();
    speechSynthesis.cancel();
    
    isSpeaking = true;
    setStatus("AI Speaking", "Listening will resume automatically", "speaking");

    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = "en-US";
    utterance.rate = 1.0;
    utterance.pitch = 1.0;

    const voices = speechSynthesis.getVoices();
    const preferredVoice = voices.find(v => 
      v.lang.startsWith("en") && (v.name.includes("Female") || v.name.includes("Samantha"))
    );
    utterance.voice = preferredVoice || voices.find(v => v.lang.startsWith("en")) || voices[0];

    utterance.onend = () => {
      isSpeaking = false;
      if (interviewActive) {
        setTimeout(() => startListening(), 500);
      }
      resolve();
    };

    utterance.onerror = () => {
      isSpeaking = false;
      resolve();
    };

    speechSynthesis.speak(utterance);
  });
}

/* ================= CONTINUOUS SPEECH RECOGNITION ================= */
function initRecognition() {
  if (!("webkitSpeechRecognition" in window)) {
    alert("‚ùå Voice recognition not supported in this browser. Please use Chrome or Edge.");
    return false;
  }

  recognition = new webkitSpeechRecognition();
  recognition.lang = "en-US";
  recognition.continuous = true; // Keep listening
  recognition.interimResults = true; // Get partial results
  recognition.maxAlternatives = 1;

  let finalTranscript = "";
  let interimTranscript = "";

  recognition.onresult = (event) => {
    interimTranscript = "";
    
    for (let i = event.resultIndex; i < event.results.length; i++) {
      const transcript = event.results[i][0].transcript;
      
      if (event.results[i].isFinal) {
        finalTranscript += transcript + " ";
        
        // Reset silence timer on final result
        clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => {
          if (finalTranscript.trim()) {
            sendUserMessage(finalTranscript.trim());
            finalTranscript = "";
          }
        }, silenceTimeout);
        
      } else {
        interimTranscript += transcript;
      }
    }

    // Show what's being heard
    const currentText = finalTranscript + interimTranscript;
    if (currentText.trim()) {
      setStatus("Listening...", `"${currentText.trim()}"`, "listening");
    }
  };

  recognition.onstart = () => {
    isListening = true;
    setStatus("Listening", "Speak naturally, I'll detect when you're done", "listening");
  };

  recognition.onend = () => {
    isListening = false;
    // Auto-restart if interview is still active and not processing
    if (interviewActive && !isProcessing && !isSpeaking) {
      setTimeout(() => recognition.start(), 300);
    }
  };

  recognition.onerror = (event) => {
    console.error("Recognition error:", event.error);
    
    if (event.error === "no-speech") {
      // Normal - just restart
      if (interviewActive && !isProcessing && !isSpeaking) {
        setTimeout(() => recognition.start(), 300);
      }
    } else if (event.error !== "aborted") {
      setStatus("Error", `Issue: ${event.error}. Restarting...`, "processing");
      if (interviewActive) {
        setTimeout(() => recognition.start(), 1000);
      }
    }
  };

  return true;
}

/* ================= SEND USER MESSAGE ================= */
async function sendUserMessage(text) {
  if (!text.trim() || isProcessing) return;
  
  recognition.stop();
  isProcessing = true;
  clearTimeout(silenceTimer);

  addMessage("user", text);
  setStatus("Processing", "Thinking about your response...", "processing");

  try {
    const response = await fetch(API, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        messages: [
          ...history,
          { role: "user", content: text }
        ]
      })
    });

    const data = await response.json();
    
    history.push({ role: "user", content: text });
    history.push({ role: "assistant", content: data.reply });

    addMessage("ai", data.reply);
    await speakAI(data.reply);

  } catch (error) {
    console.error("API Error:", error);
    const errorMsg = "I'm sorry, I had trouble connecting. Let's continue.";
    addMessage("ai", errorMsg);
    await speakAI(errorMsg);
  } finally {
    isProcessing = false;
  }
}

/* ================= START LISTENING ================= */
function startListening() {
  if (isListening || isProcessing || isSpeaking) return;
  
  try {
    recognition.start();
  } catch (e) {
    console.log("Recognition already started");
  }
}

/* ================= START INTERVIEW ================= */
startBtn.onclick = async () => {
  unlockAudio();
  
  if (!initRecognition()) return;

  interviewActive = true;
  startBtn.style.display = "none";
  stopBtn.style.display = "block";
  
  setStatus("Starting Interview", "Please wait...", "processing");
  isProcessing = true;

  try {
    const response = await fetch(API, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        messages: [
          { 
            role: "system", 
            content: "You are a friendly, professional interviewer. Ask clear, concise questions one at a time. Keep responses under 30 words when possible." 
          },
          { role: "user", content: "Start the interview." }
        ]
      })
    });

    const data = await response.json();
    history.push({ role: "assistant", content: data.reply });
    
    addMessage("ai", data.reply);
    await speakAI(data.reply);

  } catch (error) {
    console.error("Start error:", error);
    const msg = "Hello! I'm ready to interview you. Let's begin with: What position are you applying for?";
    addMessage("ai", msg);
    await speakAI(msg);
  } finally {
    isProcessing = false;
  }
};

/* ================= STOP INTERVIEW ================= */
stopBtn.onclick = () => {
  interviewActive = false;
  if (recognition) recognition.stop();
  speechSynthesis.cancel();
  clearTimeout(silenceTimer);
  
  setStatus("Interview Ended", "Thank you for your time!", "");
  stopBtn.style.display = "none";
  startBtn.style.display = "block";
  startBtn.textContent = "üîÑ Start New Interview";
  
  history = [];
};

// Load voices when available
speechSynthesis.onvoiceschanged = () => {
  speechSynthesis.getVoices();
};
</script>

</body>
</html>
