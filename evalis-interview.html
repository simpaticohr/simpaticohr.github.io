<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Evalis AI Interview</title>

<script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.standard.js"></script>

<style>
body {
  margin: 0;
  height: 100vh;
  background: radial-gradient(circle at top, #1e293b, #020617);
  font-family: system-ui;
  color: #e5e7eb;
  display: flex;
  align-items: center;
  justify-content: center;
}
.card {
  width: 92%;
  max-width: 420px;
  background: rgba(2,6,23,.9);
  border-radius: 18px;
  padding: 26px;
  box-shadow: 0 30px 80px rgba(0,0,0,.6);
  text-align: center;
}
#question { font-size: 18px; min-height: 60px; margin: 16px 0; }
#status { font-size: 14px; color: #22c55e; min-height: 20px; }
.mic {
  width: 18px; height: 18px;
  border-radius: 50%;
  background: #22c55e;
  margin: 10px auto;
  opacity: .3;
}
.mic.active {
  opacity: 1;
  animation: pulse 1.2s infinite;
}
@keyframes pulse {
  0% { box-shadow: 0 0 0 0 rgba(34,197,94,.6); }
  70% { box-shadow: 0 0 0 16px rgba(34,197,94,0); }
}
button {
  width: 100%;
  padding: 14px;
  border-radius: 12px;
  border: none;
  background: #2563eb;
  color: white;
  font-size: 16px;
}
</style>
</head>

<body>
<div class="card">
  <h3>Evalis AI Interview</h3>
  <div id="question">Ready to begin</div>
  <div id="status">Tap Start to allow microphone</div>
  <div id="mic" class="mic"></div>
  <button id="startBtn">Start Interview</button>
</div>

<script>
let audioStream, vadInstance;
let isSpeaking = false;

const questionEl = document.getElementById("question");
const statusEl = document.getElementById("status");
const micEl = document.getElementById("mic");
const startBtn = document.getElementById("startBtn");

function setMic(active){
  micEl.classList.toggle("active", active);
}

const synth = window.speechSynthesis;

function speak(text){
  synth.cancel();
  const u = new SpeechSynthesisUtterance(text);
  u.lang = "en-US";
  u.onstart = () => {
    isSpeaking = true;
    setMic(false);
    statusEl.innerText = "AI speakingâ€¦";
  };
  u.onend = () => {
    isSpeaking = false;
    statusEl.innerText = "Listeningâ€¦";
  };
  questionEl.innerText = text;
  synth.speak(u);
}

const recognition =
  new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.lang = "en-US";
recognition.onresult = e => {
  const answer = e.results[0][0].transcript;
  speak(answer.length < 15
    ? "Can you explain that in more detail?"
    : "What challenges did you face there?"
  );
};

async function startInterview(){
  startBtn.disabled = true;
  statusEl.innerText = "Requesting microphoneâ€¦";

  try {
    // ðŸ”‘ MUST BE FIRST ON ANDROID
    audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });

    vadInstance = await vad.MicVAD.new({
      stream: audioStream,
      onSpeechStart(){
        if(isSpeaking) synth.cancel();
        setMic(true);
      },
      onSpeechEnd(){
        setMic(false);
        recognition.start();
      }
    });

    vadInstance.start();
    startBtn.style.display = "none";
    speak("Tell me about yourself.");

  } catch (e){
    console.error(e);
    statusEl.innerText = "Microphone permission denied";
    startBtn.disabled = false;
  }
}

startBtn.onclick = startInterview;
</script>
</body>
</html>
