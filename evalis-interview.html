l<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evalis AI | Continuous Interview</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #030305;
            --surface: #0e0e12;
            --primary: #6366f1;
            --primary-glow: rgba(99, 102, 241, 0.4);
            --accent: #a855f7;
            --text: #f1f5f9;
            --text-dim: #94a3b8;
            --danger: #ef4444;
        }

        body {
            margin: 0;
            background: var(--bg);
            color: var(--text);
            font-family: 'Inter', sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            overflow: hidden;
        }

        /* --- THE ACTIVE ORB --- */
        .orb-wrapper {
            position: relative;
            width: 300px;
            height: 300px;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 50px;
        }

        .orb {
            width: 140px;
            height: 140px;
            background: linear-gradient(135deg, var(--primary), var(--accent));
            border-radius: 50%;
            position: relative;
            z-index: 2;
            box-shadow: 0 0 50px var(--primary-glow);
            transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
        }

        /* Dynamic States */
        .orb.idle { animation: breathe 4s infinite ease-in-out; opacity: 0.5; }
        
        /* Speaking: Pulse based on volume (simulated in CSS) */
        .orb.speaking { 
            animation: speak-pulse 1s infinite;
            box-shadow: 0 0 80px var(--primary-glow);
        }

        /* Listening: Reacts to mic input */
        .orb.listening {
            background: #ffffff;
            box-shadow: 0 0 60px rgba(255,255,255,0.3);
            transform: scale(0.9);
        }

        .orb-ring {
            position: absolute;
            border-radius: 50%;
            border: 1px solid rgba(255,255,255,0.05);
            top: 50%; left: 50%;
            transform: translate(-50%, -50%);
            z-index: 0;
            pointer-events: none;
        }
        
        .ring-1 { width: 220px; height: 220px; animation: spin 20s linear infinite; border-color: rgba(99, 102, 241, 0.2); }
        .ring-2 { width: 340px; height: 340px; animation: spin-rev 30s linear infinite; border-style: dashed; }
        .ring-3 { width: 450px; height: 450px; opacity: 0; transition: opacity 0.5s; border: 1px solid var(--primary); }
        
        .orb.speaking ~ .ring-3 { opacity: 0.2; animation: ripple 2s infinite; }

        /* --- UI ELEMENTS --- */
        .interface { z-index: 10; text-align: center; width: 90%; max-width: 400px; }
        
        h1 { font-weight: 500; font-size: 1.2rem; letter-spacing: 1px; margin-bottom: 2rem; color: var(--text-dim); text-transform: uppercase; }

        .transcript-box {
            height: 60px;
            overflow: hidden;
            font-size: 0.9rem;
            color: var(--text-dim);
            margin-bottom: 20px;
            font-style: italic;
            opacity: 0.7;
        }

        .controls { display: flex; gap: 10px; justify-content: center; flex-direction: column; }

        .btn {
            background: var(--surface);
            color: var(--text);
            border: 1px solid rgba(255, 255, 255, 0.1);
            padding: 16px 32px;
            border-radius: 100px;
            cursor: pointer;
            font-weight: 600;
            font-size: 1rem;
            transition: all 0.2s;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
        }
        
        .btn:hover { background: rgba(255,255,255,0.05); border-color: var(--text-dim); }
        .btn-primary { background: var(--text); color: #000; border: none; }
        .btn-primary:hover { background: #e2e8f0; transform: scale(1.02); }
        .btn-danger { color: var(--danger); border-color: rgba(239, 68, 68, 0.3); }

        .file-upload {
            position: relative;
            overflow: hidden;
            margin-bottom: 10px;
        }
        .file-upload input { position: absolute; left: 0; top: 0; opacity: 0; width: 100%; height: 100%; cursor: pointer; }

        /* Animations */
        @keyframes breathe { 0%, 100% { transform: scale(1); } 50% { transform: scale(1.03); } }
        @keyframes speak-pulse { 0% { transform: scale(1); } 50% { transform: scale(1.1); } 100% { transform: scale(1); } }
        @keyframes spin { 100% { transform: translate(-50%, -50%) rotate(360deg); } }
        @keyframes spin-rev { 100% { transform: translate(-50%, -50%) rotate(-360deg); } }
        @keyframes ripple { 0% { width: 140px; height: 140px; opacity: 0.5; } 100% { width: 400px; height: 400px; opacity: 0; } }

        .hidden { display: none !important; }
    </style>
</head>
<body>

    <div class="orb-wrapper">
        <div class="orb-ring ring-1"></div>
        <div class="orb-ring ring-2"></div>
        <div class="orb-ring ring-3"></div>
        <div id="orb" class="orb idle"></div>
    </div>

    <div class="interface">
        <h1 id="statusText">AI INTERVIEWER</h1>

        <div id="transcript" class="transcript-box"></div>

        <div id="setupScreen" class="controls">
            <div class="file-upload btn">
                <span id="fileName">Upload Resume (PDF)</span>
                <input type="file" id="cvInput" accept=".pdf">
            </div>
            <button id="startBtn" class="btn btn-primary hidden" onclick="startSession()">
                Start Interview
            </button>
        </div>

        <div id="activeScreen" class="controls hidden">
            <button class="btn btn-danger" onclick="endSession()">End Session</button>
        </div>
    </div>

<script>
    // ==========================================
    // CONFIGURATION
    // ==========================================
    // Set to true to use your real Worker URL, false to use the built-in mock AI for testing
    const USE_REAL_BACKEND = false; 
    const WORKER_URL = "https://evalis-ai.simpaticohrconsultancy.workers.dev";
    
    // ==========================================
    // STATE MANAGEMENT
    // ==========================================
    const state = {
        isSessionActive: false,
        isAiSpeaking: false,
        cvText: "",
        history: [],
        speechRecognition: null,
        synthesis: window.speechSynthesis,
        voice: null
    };

    // DOM Elements
    const orb = document.getElementById('orb');
    const statusText = document.getElementById('statusText');
    const transcriptEl = document.getElementById('transcript');

    // ==========================================
    // AUDIO ENGINE (The Heart of Interruption)
    // ==========================================
    
    function initSpeechRecognition() {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            alert("Browser not supported. Please use Chrome.");
            return null;
        }

        const recognition = new SpeechRecognition();
        recognition.continuous = true; // Keep listening even when silent
        recognition.interimResults = true; // CRITICAL: Allows detecting speech start immediately
        recognition.lang = 'en-US';

        recognition.onstart = () => {
            console.log("Mic Active");
        };

        recognition.onresult = (event) => {
            if (!state.isSessionActive) return;

            // 1. Detect Interruption Logic
            if (state.isAiSpeaking) {
                console.log(">>> INTERRUPTION DETECTED");
                state.synthesis.cancel(); // Stop AI immediately
                state.isAiSpeaking = false;
                orb.classList.remove('speaking');
                orb.classList.add('listening');
            }

            let finalTranscript = '';
            let interimTranscript = '';

            for (let i = event.resultIndex; i < event.results.length; ++i) {
                if (event.results[i].isFinal) {
                    finalTranscript += event.results[i][0].transcript;
                } else {
                    interimTranscript += event.results[i][0].transcript;
                }
            }

            // Visual feedback
            transcriptEl.textContent = interimTranscript || finalTranscript;
            
            // If we have a final sentence, send to AI
            if (finalTranscript.trim().length > 0) {
                // Short pause to ensure user finished sentence
                handleUserUnput(finalTranscript);
            }
        };

        recognition.onerror = (event) => {
            console.warn("Speech Error:", event.error);
            // Auto-restart if it crashes
            if (state.isSessionActive && event.error !== 'aborted') {
                setTimeout(() => recognition.start(), 100);
            }
        };

        return recognition;
    }

    function speak(text) {
        if (!state.isSessionActive) return;

        // Cancel any current speech
        state.synthesis.cancel();

        const utterance = new SpeechSynthesisUtterance(text);
        
        // Select best voice (Prioritize Google US English or similar natural voices)
        const voices = state.synthesis.getVoices();
        const preferredVoice = voices.find(v => v.name.includes("Google US English")) || 
                               voices.find(v => v.name.includes("Natural")) || 
                               voices[0];
        
        utterance.voice = preferredVoice;
        utterance.rate = 1.1; // Slightly faster for conversational feel
        utterance.pitch = 1.0;

        utterance.onstart = () => {
            state.isAiSpeaking = true;
            orb.className = "orb speaking";
            transcriptEl.textContent = ""; // Clear user text
        };

        utterance.onend = () => {
            state.isAiSpeaking = false;
            if (state.isSessionActive) {
                orb.className = "orb listening";
            } else {
                orb.className = "orb idle";
            }
        };

        state.synthesis.speak(utterance);
    }

    // ==========================================
    // LOGIC & NETWORKING
    // ==========================================

    async function handleUserUnput(text) {
        if (!text) return;
        
        console.log("User said:", text);
        orb.className = "orb idle"; // Thinking state
        
        // Add user msg to history
        state.history.push({role: "user", content: text});

        try {
            let aiResponseText = "";

            if (USE_REAL_BACKEND) {
                // --- REAL BACKEND CONNECTION ---
                const fd = new FormData();
                fd.append("history", JSON.stringify(state.history));
                fd.append("text", text);
                fd.append("cvText", state.cvText);
                
                const res = await fetch(WORKER_URL, { method: "POST", body: fd });
                const data = await res.json();
                aiResponseText = data.reply;
            } else {
                // --- MOCK AI (For Demo) ---
                aiResponseText = await mockAiEngine(text);
            }

            // Update history and Speak
            state.history.push({role: "assistant", content: aiResponseText});
            speak(aiResponseText);

        } catch (e) {
            console.error(e);
            speak("I'm having trouble connecting. Could you repeat that?");
        }
    }

    // Simulates an intelligent LLM for demo purposes
    function mockAiEngine(input) {
        return new Promise(resolve => {
            setTimeout(() => {
                const responses = [
                    "That's a fascinating point. Tell me more about how you handled the scaling challenges.",
                    "I see. Given that experience, how would you approach a similar problem in a team of ten?",
                    "Interesting. Moving on, what do you consider your biggest technical weakness right now?",
                    "Could you elaborate on the specific tools you used for that project?"
                ];
                const random = responses[Math.floor(Math.random() * responses.length)];
                resolve(random);
            }, 800); // Simulate network delay
        });
    }

    // ==========================================
    // SESSION CONTROL
    // ==========================================

    async function startSession() {
        if (!state.cvText) {
            alert("Please wait for the resume to finish processing.");
            return;
        }

        // UI Updates
        document.getElementById('setupScreen').classList.add('hidden');
        document.getElementById('activeScreen').classList.remove('hidden');
        statusText.textContent = "Interview Active";
        state.isSessionActive = true;

        // Init Voice
        state.speechRecognition = initSpeechRecognition();
        state.speechRecognition.start();

        // Initial Greeting
        orb.className = "orb idle"; // Thinking
        
        // Send "start" signal to backend or mock
        const intro = "Hello! I've reviewed your resume. It looks impressive. Shall we start by you telling me a bit about your most recent role?";
        state.history.push({role: "assistant", content: intro});
        speak(intro);
    }

    function endSession() {
        state.isSessionActive = false;
        state.synthesis.cancel();
        if (state.speechRecognition) state.speechRecognition.stop();
        
        orb.className = "orb idle";
        statusText.textContent = "Session Ended";
        document.getElementById('activeScreen').classList.add('hidden');
        document.getElementById('setupScreen').classList.remove('hidden');
        
        // Reset History
        state.history = [];
    }

    // ==========================================
    // FILE UPLOAD HANDLING
    // ==========================================
    pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js";

    document.getElementById('cvInput').onchange = async (e) => {
        const file = e.target.files[0];
        if(!file) return;
        
        document.getElementById('fileName').textContent = "Processing...";
        
        try {
            const buffer = await file.arrayBuffer();
            const pdf = await pdfjsLib.getDocument(buffer).promise;
            let fullText = "";
            
            for (let i = 1; i <= pdf.numPages; i++) {
                const page = await pdf.getPage(i);
                const textContent = await page.getTextContent();
                fullText += textContent.items.map(s => s.str).join(" ");
            }
            
            state.cvText = fullText;
            document.getElementById('fileName').textContent = "Resume Ready âœ“";
            document.getElementById('startBtn').classList.remove('hidden');
        } catch (err) {
            console.error(err);
            document.getElementById('fileName').textContent = "Error reading PDF";
        }
    };

    // Pre-load voices
    window.speechSynthesis.getVoices();

</script>
</body>
</html>
