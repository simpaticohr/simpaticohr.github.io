<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Evalis AI</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.wasm.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.standard.js"></script>
    <style>
        body { margin: 0; display: flex; justify-content: center; align-items: center; height: 100vh; background: #0c1b3a; font-family: sans-serif; color: white; }
        .card { text-align: center; padding: 20px; border-radius: 15px; background: rgba(255,255,255,0.05); }
        button { padding: 10px 20px; background: #3b82f6; color: white; border: none; border-radius: 5px; cursor: pointer; }
    </style>
</head>
<body>
    <div class="card">
        <h3 id="display">Ready?</h3>
        <p id="status">Click below to start</p>
        <button id="startBtn">Start Interview</button>
    </div>

<script>
const WORKER_URL = "https://evalis-ai.simpaticohrconsultancy.workers.dev";
const synth = window.speechSynthesis;
let isTalking = false;

async function start() {
    try {
        document.getElementById('startBtn').style.display = 'none';
        document.getElementById('status').innerText = "Initializing Microphone...";

        // Initialize Continuous Listening (VAD)
        const myvad = await vad.MicVAD.new({
            onSpeechStart: () => {
                if (isTalking) {
                    synth.cancel(); // Interrupts AI
                    isTalking = false;
                }
            },
            onSpeechEnd: (audio) => {
                // Trigger Browser Recognition after you stop talking
                recognition.start();
            }
        });
        myvad.start();
        
        speak("Hello! I am Zara. Let's start the interview. Can you tell me about your background?");
    } catch (err) {
        console.error(err);
        document.getElementById('status').innerText = "Error: Allow Microphone access.";
    }
}

// Browser Recognition for turning your voice to text
const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.onresult = async (event) => {
    const text = event.results[0][0].transcript;
    document.getElementById('status').innerText = "Thinking...";
    
    const response = await fetch(WORKER_URL, {
        method: 'POST',
        body: JSON.stringify({ answer: text })
    });
    const data = await response.json();
    speak(data.reply || data.followUp);
};

function speak(text) {
    synth.cancel();
    const utter = new SpeechSynthesisUtterance(text);
    utter.onstart = () => { isTalking = true; document.getElementById('status').innerText = "AI Speaking..."; };
    utter.onend = () => { isTalking = false; document.getElementById('status').innerText = "Listening..."; };
    document.getElementById('display').innerText = text;
    synth.speak(utter);
}

document.getElementById('startBtn').onclick = start;
</script>
</body>
</html>
