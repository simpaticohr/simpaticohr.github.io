<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Evalis AI | Next-Gen Interview</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #030305;
            --surface: #0f1016;
            --primary: #6366f1;
            --accent: #a855f7;
            --text: #f8fafc;
            --text-dim: #94a3b8;
        }

        body {
            margin: 0; background: var(--bg); color: var(--text);
            font-family: 'Inter', sans-serif; height: 100vh;
            display: flex; flex-direction: column; align-items: center; justify-content: center;
            overflow: hidden;
        }

        /* --- THE ORB & VISUALIZER --- */
        .orb-container { position: relative; width: 300px; height: 300px; display: flex; align-items: center; justify-content: center; }
        
        #visualizer-canvas {
            position: absolute; width: 100%; height: 100%;
            z-index: 1; pointer-events: none;
        }

        .orb {
            width: 130px; height: 130px;
            background: linear-gradient(135deg, var(--primary), var(--accent));
            border-radius: 50%;
            box-shadow: 0 0 50px rgba(99, 102, 241, 0.3);
            position: relative; z-index: 2;
            transition: all 0.5s cubic-bezier(0.175, 0.885, 0.32, 1.275);
        }

        /* Mode Styles */
        .listening .orb { transform: scale(1.1); background: #10b981; box-shadow: 0 0 70px rgba(16, 185, 129, 0.4); }
        .thinking .orb { transform: scale(0.8); border-radius: 35%; animation: rotate 2s linear infinite; background: #f59e0b; }
        .speaking .orb { animation: pulse 1s ease-in-out infinite; box-shadow: 0 0 80px rgba(168, 85, 247, 0.5); }

        @keyframes rotate { 100% { transform: scale(0.8) rotate(360deg); } }
        @keyframes pulse { 0%, 100% { transform: scale(1); } 50% { transform: scale(1.15); } }

        /* --- UI ELEMENTS --- */
        .interface { width: 90%; max-width: 400px; text-align: center; z-index: 10; margin-top: 20px; }
        .status-pill { 
            background: rgba(255, 255, 255, 0.03); border: 1px solid rgba(255, 255, 255, 0.1);
            padding: 8px 20px; border-radius: 50px; font-size: 0.8rem; color: var(--text-dim);
            text-transform: uppercase; letter-spacing: 1.5px; margin-bottom: 20px; display: inline-block;
        }

        .timer { font-family: monospace; font-size: 1.5rem; color: var(--text-dim); margin-top: 10px; }
        .btn { 
            width: 100%; padding: 16px; border-radius: 12px; border: none;
            font-weight: 600; cursor: pointer; transition: 0.3s; margin-top: 15px;
        }
        .btn-primary { background: var(--primary); color: white; box-shadow: 0 4px 15px rgba(99, 102, 241, 0.3); }
        .btn-danger { background: rgba(239, 68, 68, 0.1); color: #ef4444; border: 1px solid rgba(239, 68, 68, 0.2); }

        .file-upload { 
            border: 2px dashed rgba(255, 255, 255, 0.1); padding: 40px 20px; border-radius: 20px;
            cursor: pointer; color: var(--text-dim); display: block;
        }

        .hidden { display: none !important; }

        /* Report Modal */
        .modal { 
            position: fixed; inset: 0; background: rgba(0,0,0,0.95); 
            display: none; align-items: center; justify-content: center; z-index: 1000; padding: 20px;
        }
        .modal-content { background: var(--surface); padding: 30px; border-radius: 24px; border: 1px solid #222; width: 100%; max-width: 500px; }
    </style>
</head>
<body>

    <div class="orb-container" id="stage">
        <canvas id="visualizer-canvas"></canvas>
        <div id="orb" class="orb"></div>
    </div>

    <div class="interface">
        <div id="status" class="status-pill">Ready</div>
        
        <div id="setup-panel">
            <label class="file-upload">
                <span id="file-label">ðŸ“‚ Tap to upload Resume (PDF)</span>
                <input type="file" id="cvInput" accept=".pdf" class="hidden">
            </label>
            <button id="startBtn" class="btn btn-primary hidden">Begin Interview</button>
        </div>

        <div id="active-panel" class="hidden">
            <div id="timer" class="timer">15:00</div>
            <button id="endBtn" class="btn btn-danger">End Session</button>
        </div>
    </div>

    <div id="reportModal" class="modal">
        <div class="modal-content">
            <h2 style="margin-top:0">Interview Report</h2>
            <div id="reportText" style="white-space: pre-wrap; color: var(--text-dim); font-size: 0.9rem; line-height: 1.6;"></div>
            <button onclick="location.reload()" class="btn btn-primary">Dismiss</button>
        </div>
    </div>

<script>
    const API_URL = "https://evalis-ai.simpaticohrconsultancy.workers.dev";
    pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js";
    const isAndroid = typeof Android !== "undefined";

    let state = {
        cvText: "",
        history: [],
        timeLeft: 900,
        isStarted: false,
        isThinking: false
    };

    let timerInterval;
    let audioContext, analyser, dataArray, canvas, ctx;

    // --- 1. THE SOUND WAVE (PRODUCTION Polish) ---
    function initVisualizer() {
        canvas = document.getElementById('visualizer-canvas');
        ctx = canvas.getContext('2d');
        canvas.width = 300;
        canvas.height = 300;

        navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const source = audioContext.createMediaStreamSource(stream);
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 64;
            source.connect(analyser);
            
            dataArray = new Uint8Array(analyser.frequencyBinCount);
            draw();
        }).catch(err => console.log("Visualizer blocked or no mic found."));
    }

    function draw() {
        requestAnimationFrame(draw);
        if (!analyser) return;
        analyser.getByteFrequencyData(dataArray);

        ctx.clearRect(0, 0, canvas.width, canvas.height);
        const centerX = canvas.width / 2;
        const centerY = canvas.height / 2;
        const radius = 75;

        ctx.beginPath();
        for (let i = 0; i < dataArray.length; i++) {
            const barHeight = dataArray[i] * 0.4;
            const angle = (i * 2 * Math.PI) / dataArray.length;
            const x = centerX + Math.cos(angle) * (radius + barHeight);
            const y = centerY + Math.sin(angle) * (radius + barHeight);
            
            if (i === 0) ctx.moveTo(x, y);
            else ctx.lineTo(x, y);
        }
        ctx.closePath();
        ctx.strokeStyle = getComputedStyle(document.documentElement).getPropertyValue('--primary');
        ctx.lineWidth = 2;
        ctx.stroke();
    }

    // --- 2. THE INTERVIEW ENGINE ---
    function setUI(mode) {
        const orb = document.getElementById('orb');
        const stage = document.getElementById('stage');
        const status = document.getElementById('status');
        
        stage.className = "orb-container " + mode;
        if(mode === 'listening') status.textContent = "Listening...";
        else if(mode === 'thinking') status.textContent = "Analyzing...";
        else if(mode === 'speaking') status.textContent = "AI Speaking...";
    }

    async function handleResponse(text) {
        if (!state.isStarted || state.isThinking || !text.trim()) return;
        
        state.isThinking = true;
        setUI('thinking');

        const fd = new FormData();
        fd.append("mode", "interview");
        fd.append("history", JSON.stringify(state.history));
        fd.append("text", text);
        fd.append("cvText", state.cvText);

        try {
            const res = await fetch(API_URL, { method: "POST", body: fd });
            const data = await res.json();
            state.history = data.history;
            state.isThinking = false;
            speak(data.reply);
        } catch (e) {
            state.isThinking = false;
            startListening(); // Fallback to listen again
        }
    }

    function startListening() {
        setUI('listening');
        if (isAndroid) Android.startInterview();
        else {
            try { recognition.start(); } catch(e) {}
        }
    }

    function speak(text) {
        setUI('speaking');
        if (isAndroid) Android.speak(text);
        else {
            const u = new SpeechSynthesisUtterance(text);
            u.rate = 1.0;
            u.onend = () => onFinishedSpeaking();
            window.speechSynthesis.speak(u);
        }
    }

    window.onRecognitionResult = (t) => handleResponse(t);
    window.onFinishedSpeaking = () => { if(state.isStarted) startListening(); };

    // --- 3. SYSTEM CONTROLS ---
    document.getElementById('cvInput').onchange = async (e) => {
        const file = e.target.files[0];
        if(!file) return;
        
        document.getElementById('file-label').textContent = "Processing PDF...";
        const buffer = await file.arrayBuffer();
        const pdf = await pdfjsLib.getDocument(buffer).promise;
        let text = "";
        for (let i = 1; i <= pdf.numPages; i++) {
            const page = await pdf.getPage(i);
            const content = await page.getTextContent();
            text += content.items.map(s => s.str).join(" ");
        }
        state.cvText = text;
        document.getElementById('file-label').textContent = "Resume Ready âœ“";
        document.getElementById('startBtn').classList.remove('hidden');
    };

    document.getElementById('startBtn').onclick = async () => {
        state.isStarted = true;
        initVisualizer();
        if (audioContext) audioContext.resume(); // CRITICAL: Resume audio on click
        
        document.getElementById('setup-panel').classList.add('hidden');
        document.getElementById('active-panel').classList.remove('hidden');

        timerInterval = setInterval(() => {
            state.timeLeft--;
            const m = Math.floor(state.timeLeft / 60);
            const s = state.timeLeft % 60;
            document.getElementById('timer').textContent = `${m}:${s < 10 ? '0'+s : s}`;
            if (state.timeLeft <= 0) finish();
        }, 1000);

        setUI('thinking');
        const fd = new FormData();
        fd.append("mode", "start");
        fd.append("cvText", state.cvText);
        const res = await fetch(API_URL, { method: "POST", body: fd });
        const data = await res.json();
        state.history = data.history;
        speak(data.reply);
    };

    async function finish() {
        state.isStarted = false;
        clearInterval(timerInterval);
        setUI('thinking');
        const fd = new FormData();
        fd.append("mode", "score");
        fd.append("history", JSON.stringify(state.history));
        const res = await fetch(API_URL, { method: "POST", body: fd });
        const data = await res.json();
        document.getElementById('reportText').textContent = data.reply;
        document.getElementById('reportModal').style.display = 'flex';
    }

    document.getElementById('endBtn').onclick = finish;

    // --- 4. WAKE LOCK (STAY AWAKE) ---
    let wakeLock = null;
    async function stayAwake() {
        try { if ('wakeLock' in navigator) wakeLock = await navigator.wakeLock.request('screen'); } catch(e) {}
    }
    stayAwake();
    document.addEventListener('visibilitychange', () => { if (document.visibilityState === 'visible') stayAwake(); });

    // Web Speech PC Setup
    let recognition;
    if (!isAndroid) {
        const R = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (R) {
            recognition = new R();
            recognition.onresult = (e) => handleResponse(e.results[0][0].transcript);
        }
    }
</script>
</body>
</html>
