<script>
const startBtn = document.getElementById("startBtn");
const statusEl = document.getElementById("status");
const questionEl = document.getElementById("question");
const pulse = document.getElementById("pulse");

const synth = window.speechSynthesis;
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

const questions = [
  "Tell me about yourself.",
  "What experience do you have related to this role?",
  "Describe a challenge you handled well.",
  "Why should we hire you?"
];

let qIndex = 0;
let recognition;
let vadInstance;

function speak(text) {
  synth.cancel();
  questionEl.innerText = text;
  statusEl.innerText = "Evalis AI Speaking";
  const u = new SpeechSynthesisUtterance(text);
  u.onend = () => statusEl.innerText = "Listening…";
  synth.speak(u);
}

startBtn.onclick = () => {
  startBtn.style.display = "none";
  statusEl.innerText = "Requesting microphone…";

  // ✅ MUST be inside click (no await before)
  navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {

    recognition = new SpeechRecognition();
    recognition.lang = "en-US";

    recognition.onresult = () => {
      recognition.stop();
      qIndex++;
      if (qIndex < questions.length) {
        speak(questions[qIndex]);
      } else {
        statusEl.innerText = "Interview Completed";
        questionEl.innerText = "Thank you. Interview complete.";
        pulse.style.display = "none";
      }
    };

    vad.MicVAD.new({
      stream,
      onSpeechStart: () => {
        pulse.style.display = "block";
        if (synth.speaking) synth.cancel();
      },
      onSpeechEnd: () => {
        pulse.style.display = "none";
        try { recognition.start(); } catch(e){}
      }
    }).then(v => {
      vadInstance = v;
      vadInstance.start();
      speak(questions[0]);
    });

  }).catch(() => {
    statusEl.innerText = "Microphone blocked";
    questionEl.innerText = "Please allow mic access and refresh.";
  });
};
</script>
