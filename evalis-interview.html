<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Evalis AI - Elite Live Interview</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=DM+Mono:wght@400;500&family=Syne:wght@700;800&display=swap');

        :root {
            --primary: #6366f1;
            --primary-dim: rgba(99,102,241,0.15);
            --danger: #ef4444;
            --bg: #020617;
            --surface: rgba(255,255,255,0.04);
            --border: rgba(255,255,255,0.08);
            --text-muted: #64748b;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; -webkit-tap-highlight-color: transparent; }

        body {
            background: var(--bg);
            color: white;
            font-family: 'DM Mono', monospace;
            display: flex;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            padding: 16px;
            overflow-x: hidden;
        }

        body::before {
            content: '';
            position: fixed;
            inset: 0;
            background: radial-gradient(ellipse 60% 40% at 50% 0%, rgba(99,102,241,0.12) 0%, transparent 70%);
            pointer-events: none;
        }

        .card {
            width: 100%;
            max-width: 460px;
            background: var(--surface);
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            border-radius: 20px;
            padding: 28px 24px;
            border: 1px solid var(--border);
            box-shadow: 0 30px 60px rgba(0,0,0,0.6);
            position: relative;
        }

        .brand {
            font-family: 'Syne', sans-serif;
            font-size: 13px;
            font-weight: 700;
            letter-spacing: 3px;
            color: var(--primary);
            text-transform: uppercase;
            margin-bottom: 20px;
            opacity: 0.8;
        }

        #timer {
            font-family: 'Syne', sans-serif;
            font-size: 52px;
            font-weight: 800;
            color: white;
            letter-spacing: -2px;
            line-height: 1;
            margin-bottom: 4px;
        }

        #timer.warning { color: var(--danger); }

        #status {
            font-size: 11px;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 2px;
            margin-bottom: 20px;
            height: 16px;
        }

        .status-dot {
            display: inline-block;
            width: 6px; height: 6px;
            border-radius: 50%;
            background: var(--primary);
            margin-right: 6px;
            animation: pulse 1.5s infinite;
            vertical-align: middle;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; transform: scale(1); }
            50% { opacity: 0.4; transform: scale(0.8); }
        }

        .wave-box {
            height: 60px;
            margin: 16px 0;
            border-radius: 10px;
            overflow: hidden;
            background: rgba(0,0,0,0.2);
            display: flex;
            align-items: center;
            justify-content: center;
        }

        canvas { width: 100%; height: 60px; display: block; }

        .upload-area {
            border: 1.5px dashed var(--border);
            border-radius: 12px;
            padding: 20px;
            text-align: center;
            margin-bottom: 16px;
            cursor: pointer;
            transition: border-color 0.2s, background 0.2s;
            position: relative;
        }

        .upload-area:hover, .upload-area.ready {
            border-color: var(--primary);
            background: var(--primary-dim);
        }

        .upload-area input {
            position: absolute;
            inset: 0;
            opacity: 0;
            cursor: pointer;
            width: 100%;
            height: 100%;
        }

        .upload-icon { font-size: 28px; margin-bottom: 6px; }
        .upload-label { font-size: 12px; color: var(--text-muted); }
        .upload-name { font-size: 12px; color: var(--primary); margin-top: 4px; }

        .btn {
            width: 100%;
            padding: 16px;
            border-radius: 12px;
            border: none;
            font-family: 'DM Mono', monospace;
            font-weight: 500;
            font-size: 14px;
            letter-spacing: 1px;
            cursor: pointer;
            transition: all 0.2s;
            position: relative;
            overflow: hidden;
        }

        .btn-primary {
            background: var(--primary);
            color: white;
        }

        .btn-primary:disabled {
            opacity: 0.3;
            cursor: not-allowed;
        }

        .btn-primary:not(:disabled):active {
            transform: scale(0.97);
        }

        .btn-danger {
            background: rgba(239,68,68,0.1);
            color: var(--danger);
            border: 1px solid rgba(239,68,68,0.3);
            display: none;
            margin-top: 10px;
        }

        .btn-danger:active { transform: scale(0.97); }

        /* PTT button for Android */
        .btn-ptt {
            background: rgba(99,102,241,0.1);
            color: var(--primary);
            border: 1px solid rgba(99,102,241,0.4);
            margin-top: 10px;
            display: none;
            padding: 20px;
            font-size: 13px;
            user-select: none;
            -webkit-user-select: none;
        }

        .btn-ptt.recording {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
            box-shadow: 0 0 20px rgba(99,102,241,0.5);
        }

        #log {
            margin-top: 20px;
            max-height: 220px;
            overflow-y: auto;
            border-top: 1px solid var(--border);
            padding-top: 16px;
            scroll-behavior: smooth;
        }

        .log-entry {
            margin-bottom: 14px;
            font-size: 13px;
            line-height: 1.5;
            animation: fadeIn 0.3s ease;
        }

        .log-entry.ai { color: #a5b4fc; }
        .log-entry.user { color: #94a3b8; }
        .log-label { font-size: 10px; letter-spacing: 1.5px; opacity: 0.6; margin-bottom: 2px; }

        @keyframes fadeIn { from { opacity: 0; transform: translateY(4px); } to { opacity: 1; transform: translateY(0); } }

        .error-banner {
            background: rgba(239,68,68,0.1);
            border: 1px solid rgba(239,68,68,0.3);
            color: #fca5a5;
            border-radius: 10px;
            padding: 12px 14px;
            font-size: 12px;
            margin-bottom: 14px;
            line-height: 1.5;
            display: none;
        }

        .report-wrap {
            width: 100%;
            max-width: 600px;
            background: var(--surface);
            border-radius: 20px;
            padding: 28px 24px;
            border: 1px solid var(--border);
        }

        .report-title {
            font-family: 'Syne', sans-serif;
            font-size: 22px;
            font-weight: 800;
            color: var(--primary);
            margin-bottom: 16px;
        }

        .report-body {
            background: var(--primary-dim);
            border: 1px solid rgba(99,102,241,0.25);
            border-radius: 12px;
            padding: 20px;
            font-size: 13px;
            line-height: 1.8;
            white-space: pre-wrap;
            color: #e2e8f0;
        }

        .spinner {
            display: inline-block;
            width: 14px; height: 14px;
            border: 2px solid rgba(255,255,255,0.2);
            border-top-color: white;
            border-radius: 50%;
            animation: spin 0.7s linear infinite;
            vertical-align: middle;
            margin-right: 8px;
        }

        @keyframes spin { to { transform: rotate(360deg); } }

        /* Flat bars when no mic */
        .idle-bars {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 3px;
            height: 100%;
            padding: 0 10px;
        }

        .idle-bar {
            width: 3px;
            border-radius: 2px;
            background: rgba(99,102,241,0.3);
            height: 8px;
        }
    </style>
</head>
<body>

<div class="card" id="mainCard">
    <div class="brand">Evalis AI</div>
    <div id="timer">15:00</div>
    <div id="status">Upload Resume to Start</div>

    <div class="wave-box" id="waveBox">
        <div class="idle-bars" id="idleBars">
            <!-- static bars shown before interview -->
            <div class="idle-bar"></div><div class="idle-bar"></div><div class="idle-bar"></div>
            <div class="idle-bar"></div><div class="idle-bar"></div><div class="idle-bar"></div>
            <div class="idle-bar"></div><div class="idle-bar"></div>
        </div>
        <canvas id="canvas" style="display:none;"></canvas>
    </div>

    <div class="error-banner" id="errorBanner"></div>

    <div class="upload-area" id="uploadArea">
        <input type="file" id="cvInput" accept=".pdf" aria-label="Upload PDF resume">
        <div class="upload-icon">ðŸ“„</div>
        <div class="upload-label">Tap to upload your resume (PDF)</div>
        <div class="upload-name" id="uploadName"></div>
    </div>

    <button id="startBtn" class="btn btn-primary" disabled>Start Interview</button>
    <button id="pttBtn" class="btn btn-ptt" id="pttBtn">ðŸŽ™ Hold to Speak</button>
    <button id="endBtn" class="btn btn-danger">End Interview</button>

    <div id="log"></div>
</div>

<script>
    const API = "https://evalis-ai.simpaticohrconsultancy.workers.dev";
    const pdfjsLib = window['pdfjs-dist/build/pdf'];
    pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js";

    // â”€â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let cvText = "", history = [], interviewActive = false, isAISpeaking = false;
    let audioCtx, analyser, dataArray, canvasCtx, timerInterval, timeLeft = 15 * 60;
    let mediaStream, mediaRecorder, audioChunks = [];
    let recognition = null;
    const isAndroid = /android/i.test(navigator.userAgent);
    const isIOS = /iphone|ipad|ipod/i.test(navigator.userAgent);
    const hasSR = ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) && !isAndroid;

    // â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    function showError(msg) {
        const b = document.getElementById("errorBanner");
        b.textContent = msg;
        b.style.display = "block";
        setTimeout(() => b.style.display = "none", 6000);
    }

    function setStatus(msg, dot = false) {
        document.getElementById("status").innerHTML =
            (dot ? '<span class="status-dot"></span>' : '') + msg;
    }

    function addLog(role, text) {
        const log = document.getElementById("log");
        const el = document.createElement("div");
        el.className = `log-entry ${role}`;
        el.innerHTML = `<div class="log-label">${role === 'ai' ? 'AI' : 'YOU'}</div>${text}`;
        log.prepend(el);
    }

    // â”€â”€â”€ PDF Loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    document.getElementById("cvInput").onchange = async e => {
        const file = e.target.files[0];
        if (!file) return;
        setStatus("Reading resume...", true);
        try {
            const pdf = await pdfjsLib.getDocument(await file.arrayBuffer()).promise;
            let text = "";
            for (let i = 1; i <= pdf.numPages; i++) {
                const page = await pdf.getPage(i);
                text += (await page.getTextContent()).items.map(s => s.str).join(" ") + " ";
            }
            cvText = text;
            document.getElementById("startBtn").disabled = false;
            document.getElementById("uploadName").textContent = "âœ“ " + file.name;
            document.getElementById("uploadArea").classList.add("ready");
            setStatus("Resume Ready");
        } catch (err) {
            showError("Failed to read PDF. Please try another file.");
            setStatus("Upload Resume to Start");
        }
    };

    // â”€â”€â”€ Waveform â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    function initWave(stream) {
        try {
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioCtx.createAnalyser();
            analyser.fftSize = 256;
            audioCtx.createMediaStreamSource(stream).connect(analyser);
            dataArray = new Uint8Array(analyser.frequencyBinCount);
            const cv = document.getElementById("canvas");
            cv.width = cv.offsetWidth * window.devicePixelRatio || 400;
            cv.height = 60;
            canvasCtx = cv.getContext("2d");
            document.getElementById("idleBars").style.display = "none";
            cv.style.display = "block";

            const draw = () => {
                if (!interviewActive) return;
                requestAnimationFrame(draw);
                analyser.getByteFrequencyData(dataArray);
                canvasCtx.clearRect(0, 0, cv.width, cv.height);
                const barW = 4, gap = 3, total = barW + gap;
                const count = Math.floor(cv.width / total);
                for (let i = 0; i < count; i++) {
                    const val = dataArray[Math.floor(i * dataArray.length / count)];
                    const h = Math.max(4, (val / 255) * 50);
                    canvasCtx.fillStyle = isAISpeaking ? "#818cf8" : "#6366f1";
                    canvasCtx.beginPath();
                    canvasCtx.roundRect(i * total, (60 - h) / 2, barW, h, 2);
                    canvasCtx.fill();
                }
            };
            draw();
        } catch (e) {
            console.warn("Waveform init failed:", e);
        }
    }

    // â”€â”€â”€ Timer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    function startTimer() {
        timerInterval = setInterval(() => {
            timeLeft--;
            const min = Math.floor(timeLeft / 60);
            const sec = timeLeft % 60;
            const el = document.getElementById("timer");
            el.textContent = `${min}:${sec < 10 ? '0' : ''}${sec}`;
            if (timeLeft <= 60) el.classList.add("warning");
            if (timeLeft <= 0) finishInterview();
        }, 1000);
    }

    // â”€â”€â”€ TTS (cross-platform safe) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    function speak(text, onDone) {
        window.speechSynthesis.cancel();

        // Android/WebView: split long text into chunks to avoid Android TTS cut-off bug
        const chunks = splitTTS(text);
        let i = 0;

        function sayNext() {
            if (i >= chunks.length) {
                isAISpeaking = false;
                if (onDone) onDone();
                return;
            }
            const u = new SpeechSynthesisUtterance(chunks[i++]);
            u.rate = 0.95;
            u.pitch = 1;
            u.volume = 1;

            // Force voice selection on Android for reliability
            const voices = window.speechSynthesis.getVoices();
            const best = voices.find(v => v.lang.startsWith("en") && !v.localService === false)
                       || voices.find(v => v.lang.startsWith("en"))
                       || voices[0];
            if (best) u.voice = best;

            u.onend = sayNext;
            u.onerror = () => { i = chunks.length; isAISpeaking = false; if (onDone) onDone(); };
            window.speechSynthesis.speak(u);
        }

        isAISpeaking = true;
        sayNext();
        addLog('ai', text);
    }

    // Android TTS crashes on long utterances â€” chunk at sentence boundaries
    function splitTTS(text) {
        const sentences = text.match(/[^.!?]+[.!?]*/g) || [text];
        const chunks = [];
        let cur = "";
        for (const s of sentences) {
            if ((cur + s).length > 180) {
                if (cur) chunks.push(cur.trim());
                cur = s;
            } else {
                cur += s;
            }
        }
        if (cur.trim()) chunks.push(cur.trim());
        return chunks.length ? chunks : [text];
    }

    // Workaround: Android Chrome pauses speechSynthesis after ~15s (Chrome bug)
    // Keep it alive with a periodic keepalive
    let ttsKeepalive;
    function startTTSKeepalive() {
        clearInterval(ttsKeepalive);
        ttsKeepalive = setInterval(() => {
            if (window.speechSynthesis.speaking) {
                window.speechSynthesis.pause();
                window.speechSynthesis.resume();
            }
        }, 10000);
    }

    // â”€â”€â”€ API call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async function sendToAPI(transcript) {
        setStatus("Thinking...", true);
        try {
            const fd = new FormData();
            fd.append("mode", "interview");
            fd.append("history", JSON.stringify(history));
            fd.append("text", transcript);
            fd.append("cvText", cvText);

            const res = await fetch(API, { method: "POST", body: fd });
            if (!res.ok) throw new Error(`HTTP ${res.status}`);
            const data = await res.json();
            history = data.history;

            setStatus("AI Speaking...", true);
            speak(data.reply, () => {
                if (interviewActive) {
                    if (hasSR) {
                        try { recognition.start(); } catch(e) {}
                    } else {
                        setStatus("Hold button to speak", true);
                        document.getElementById("pttBtn").style.display = "block";
                    }
                }
            });
        } catch (err) {
            showError("Network error. Please check your connection.");
            setStatus("Error â€” try again");
            console.error(err);
        }
    }

    // â”€â”€â”€ Speech Recognition: Desktop/iOS Safari â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    function initContinuousRecognition() {
        const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
        recognition = new SR();
        recognition.continuous = false; // false is more reliable cross-platform
        recognition.interimResults = false;
        recognition.lang = "en-US";
        recognition.maxAlternatives = 1;

        recognition.onresult = e => {
            const transcript = Array.from(e.results)
                .map(r => r[0].transcript).join(" ").trim();
            if (!transcript) return;
            if (isAISpeaking) {
                window.speechSynthesis.cancel();
                isAISpeaking = false;
            }
            addLog('user', transcript);
            sendToAPI(transcript);
        };

        recognition.onerror = e => {
            if (e.error === "not-allowed") {
                showError("Microphone access denied. Please allow microphone in browser settings.");
                setStatus("Mic blocked");
                return;
            }
            if (e.error === "no-speech") {
                if (interviewActive && !isAISpeaking) {
                    try { recognition.start(); } catch(e) {}
                }
                return;
            }
            console.warn("SR error:", e.error);
        };

        recognition.onend = () => {
            if (interviewActive && !isAISpeaking) {
                setTimeout(() => {
                    try { recognition.start(); } catch(e) {}
                }, 300);
            }
        };

        try { recognition.start(); } catch(e) {}
        setStatus("Listening...", true);
    }

    // â”€â”€â”€ Push-to-Talk: Android (MediaRecorder â†’ Whisper-style via API) â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let pttRecording = false;

    function initPTT() {
        const btn = document.getElementById("pttBtn");
        btn.style.display = "block";
        btn.textContent = "ðŸŽ™ Hold to Speak";
        setStatus("Hold button to speak", true);

        // Use both touch and mouse for compatibility
        const startRec = async () => {
            if (!mediaStream) return;
            if (pttRecording || isAISpeaking) return;
            pttRecording = true;
            audioChunks = [];
            btn.classList.add("recording");
            btn.textContent = "ðŸ”´ Recording...";
            setStatus("Recording...", true);

            try {
                // Try webm/opus first (Android), fallback to default
                const mimeType = MediaRecorder.isTypeSupported("audio/webm;codecs=opus")
                    ? "audio/webm;codecs=opus"
                    : MediaRecorder.isTypeSupported("audio/webm")
                    ? "audio/webm"
                    : "";
                mediaRecorder = new MediaRecorder(mediaStream, mimeType ? { mimeType } : {});
                mediaRecorder.ondataavailable = e => { if (e.data.size > 0) audioChunks.push(e.data); };
                mediaRecorder.onstop = handlePTTStop;
                mediaRecorder.start();
            } catch (err) {
                showError("Recording failed: " + err.message);
                pttRecording = false;
                btn.classList.remove("recording");
                btn.textContent = "ðŸŽ™ Hold to Speak";
            }
        };

        const stopRec = () => {
            if (!pttRecording || !mediaRecorder) return;
            pttRecording = false;
            btn.classList.remove("recording");
            btn.textContent = "ðŸŽ™ Hold to Speak";
            setStatus("Processing...", true);
            try { mediaRecorder.stop(); } catch(e) {}
        };

        btn.addEventListener("touchstart", e => { e.preventDefault(); startRec(); }, { passive: false });
        btn.addEventListener("touchend", e => { e.preventDefault(); stopRec(); }, { passive: false });
        btn.addEventListener("touchcancel", e => { e.preventDefault(); stopRec(); }, { passive: false });
        btn.addEventListener("mousedown", startRec);
        btn.addEventListener("mouseup", stopRec);
        btn.addEventListener("mouseleave", stopRec);
    }

    async function handlePTTStop() {
        if (!audioChunks.length) return;
        const blob = new Blob(audioChunks, { type: audioChunks[0].type || "audio/webm" });

        // Option A: Try Web Speech API on recorded audio (not available on Android)
        // Option B: Use the SpeechRecognition with a short manual trigger
        // Option C (best for Android): Use browser's built-in SR with one-shot mode
        // We'll try SR one-shot triggered after recording, fallback to sending audio blob to API

        // Try one-shot SR on Android â€” some newer Android Chrome versions support it
        if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
            const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
            const sr = new SR();
            sr.continuous = false;
            sr.interimResults = false;
            sr.lang = "en-US";
            let gotResult = false;

            sr.onresult = e => {
                gotResult = true;
                const t = e.results[0][0].transcript.trim();
                addLog('user', t);
                sendToAPI(t);
            };

            sr.onerror = () => {
                if (!gotResult) tryAudioAPIFallback(blob);
            };

            sr.onend = () => {
                if (!gotResult) tryAudioAPIFallback(blob);
            };

            try {
                sr.start();
                // Give SR 8 seconds to get a result from playback context
                // On Android this often still fails â€” the fallback is key
                setTimeout(() => { if (!gotResult) { sr.abort(); } }, 8000);
                return;
            } catch(e) {}
        }

        tryAudioAPIFallback(blob);
    }

    async function tryAudioAPIFallback(blob) {
        // Send audio blob to backend for transcription (if API supports it)
        // Otherwise, show a text input fallback so the session can continue
        try {
            const fd = new FormData();
            fd.append("mode", "transcribe");
            fd.append("audio", blob, "recording.webm");
            fd.append("history", JSON.stringify(history));
            fd.append("cvText", cvText);

            const res = await fetch(API, { method: "POST", body: fd });
            if (!res.ok) throw new Error("no transcription endpoint");
            const data = await res.json();

            if (data.transcript) {
                addLog('user', data.transcript);
                sendToAPI(data.transcript);
            } else if (data.reply) {
                // API accepted audio + responded directly
                history = data.history;
                speak(data.reply);
            } else {
                showTextInputFallback();
            }
        } catch(e) {
            // Final fallback: show text input
            showTextInputFallback();
        }
    }

    function showTextInputFallback() {
        setStatus("Type your response", true);
        const existing = document.getElementById("textFallback");
        if (existing) { existing.style.display = "flex"; return; }

        const wrap = document.createElement("div");
        wrap.id = "textFallback";
        wrap.style.cssText = "display:flex;gap:8px;margin-top:10px;";
        wrap.innerHTML = `
            <input id="textInput" placeholder="Type your answer..." style="
                flex:1; background:rgba(255,255,255,0.05); border:1px solid rgba(255,255,255,0.15);
                border-radius:10px; padding:12px; color:white; font-family:inherit; font-size:13px;
                outline:none;
            ">
            <button onclick="submitTextAnswer()" style="
                background:var(--primary); border:none; border-radius:10px; padding:12px 16px;
                color:white; font-family:inherit; cursor:pointer; font-size:13px;
            ">Send</button>`;

        document.getElementById("pttBtn").after(wrap);
    }

    window.submitTextAnswer = function() {
        const input = document.getElementById("textInput");
        const text = input.value.trim();
        if (!text) return;
        addLog('user', text);
        input.value = "";
        sendToAPI(text);
    };

    // â”€â”€â”€ Start Session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    document.getElementById("startBtn").onclick = async () => {
        // Unlock audio on user gesture (required on iOS/Android)
        const unlockCtx = new (window.AudioContext || window.webkitAudioContext)();
        unlockCtx.resume().catch(() => {});
        window.speechSynthesis.cancel();
        const silentU = new SpeechSynthesisUtterance("");
        silentU.volume = 0;
        window.speechSynthesis.speak(silentU);

        // Pre-load voices (Android needs this before speaking)
        if (window.speechSynthesis.getVoices().length === 0) {
            await new Promise(r => {
                window.speechSynthesis.onvoiceschanged = r;
                setTimeout(r, 1500); // give up after 1.5s
            });
        }

        // Request mic
        try {
            mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
        } catch (err) {
            if (err.name === "NotAllowedError" || err.name === "PermissionDeniedError") {
                showError("Microphone access denied. Please allow microphone permission and reload.");
            } else if (err.name === "NotFoundError") {
                showError("No microphone found on this device.");
            } else {
                showError("Microphone error: " + err.message);
            }
            return;
        }

        interviewActive = true;
        initWave(mediaStream);
        startTimer();
        startTTSKeepalive();

        document.getElementById("uploadArea").style.display = "none";
        document.getElementById("startBtn").style.display = "none";
        document.getElementById("endBtn").style.display = "block";

        setStatus("Connecting...", true);

        try {
            const fd = new FormData();
            fd.append("mode", "start");
            fd.append("cvText", cvText);
            const res = await fetch(API, { method: "POST", body: fd });
            if (!res.ok) throw new Error(`HTTP ${res.status}`);
            const data = await res.json();
            history = data.history;

            setStatus("AI Speaking...", true);
            speak(data.reply, () => {
                if (!interviewActive) return;
                if (hasSR) {
                    initContinuousRecognition();
                } else {
                    // Android â€” use PTT
                    initPTT();
                }
            });
        } catch (err) {
            showError("Failed to start interview. Check your connection.");
            setStatus("Error");
            interviewActive = false;
            console.error(err);
        }
    };

    // â”€â”€â”€ Finish & Score Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async function finishInterview() {
        interviewActive = false;
        clearInterval(timerInterval);
        clearInterval(ttsKeepalive);
        if (recognition) { try { recognition.stop(); } catch(e) {} }
        window.speechSynthesis.cancel();
        if (mediaStream) mediaStream.getTracks().forEach(t => t.stop());

        document.body.innerHTML = `
        <div class="report-wrap">
            <div class="report-title">Generating Report...</div>
            <div style="color:var(--text-muted);font-size:13px;margin-bottom:20px">
                <span class="spinner"></span>Analyzing your interview performance...
            </div>
        </div>`;

        try {
            const fd = new FormData();
            fd.append("mode", "score");
            fd.append("history", JSON.stringify(history));
            const res = await fetch(API, { method: "POST", body: fd });
            if (!res.ok) throw new Error(`HTTP ${res.status}`);
            const data = await res.json();

            document.body.innerHTML = `
            <div class="report-wrap">
                <div class="report-title">Final Score Report</div>
                <div class="report-body">${data.reply}</div>
                <button onclick="location.reload()" class="btn btn-primary" style="margin-top:20px">
                    New Session
                </button>
            </div>`;
        } catch (err) {
            document.body.innerHTML = `
            <div class="report-wrap">
                <div class="report-title" style="color:var(--danger)">Report Failed</div>
                <div style="color:var(--text-muted);font-size:13px">Could not generate report. Your session data may not have been saved.</div>
                <button onclick="location.reload()" class="btn btn-primary" style="margin-top:20px">Try Again</button>
            </div>`;
        }
    }

    document.getElementById("endBtn").onclick = finishInterview;

    // â”€â”€â”€ Prevent screen sleep on Android during interview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let wakeLock = null;
    async function requestWakeLock() {
        if ('wakeLock' in navigator) {
            try {
                wakeLock = await navigator.wakeLock.request('screen');
            } catch(e) {}
        }
    }
    document.getElementById("startBtn").addEventListener("click", requestWakeLock, { once: true });
</script>
</body>
</html>
