<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evalis AI | Intelligent Interviewer</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #05050a;
            --surface: #0f1016;
            --primary: #6366f1;
            --accent: #a855f7;
            --text: #e2e8f0;
            --text-dim: #64748b;
        }

        body {
            margin: 0; background: var(--bg); color: var(--text);
            font-family: 'Inter', sans-serif; display: flex; flex-direction: column;
            align-items: center; justify-content: center; min-height: 100vh; overflow: hidden;
        }

        /* --- ORB ANIMATIONS --- */
        .orb-container { position: relative; width: 200px; height: 200px; display: flex; align-items: center; justify-content: center; margin-bottom: 40px; }
        
        .orb {
            width: 120px; height: 120px; border-radius: 50%;
            background: linear-gradient(135deg, var(--primary), var(--accent));
            box-shadow: 0 0 60px rgba(99, 102, 241, 0.4);
            position: relative; z-index: 2; transition: all 0.3s ease;
        }

        .orb.listening { background: linear-gradient(135deg, #22c55e, #10b981); box-shadow: 0 0 60px rgba(34, 197, 94, 0.4); }
        .orb.thinking { background: linear-gradient(135deg, #f59e0b, #d97706); animation: pulse-slow 2s infinite; }
        .orb.speaking { animation: pulse-speak 0.5s infinite alternate; }

        @keyframes pulse-slow { 0%, 100% { transform: scale(1); opacity: 1; } 50% { transform: scale(0.9); opacity: 0.8; } }
        @keyframes pulse-speak { 0% { transform: scale(1); } 100% { transform: scale(1.1); } }

        /* --- UI --- */
        .interface { width: 100%; max-width: 500px; text-align: center; z-index: 10; }
        .status-pill { display: inline-block; padding: 8px 20px; background: rgba(255, 255, 255, 0.05); border-radius: 50px; font-size: 0.9rem; color: var(--text-dim); margin-bottom: 30px; border: 1px solid rgba(255,255,255,0.1); }
        .timer { font-family: monospace; color: var(--text-dim); margin-top: 15px; font-size: 1.2rem; }
        
        .btn { background: var(--surface); color: var(--text); border: 1px solid rgba(255,255,255,0.1); padding: 12px 24px; border-radius: 12px; cursor: pointer; font-weight: 600; margin-top: 10px; }
        .btn-primary { background: var(--primary); border: none; }
        .file-trigger { border: 2px dashed rgba(255,255,255,0.2); padding: 40px; border-radius: 16px; cursor: pointer; display: block; margin-bottom: 20px; }
        
        /* Modal */
        .modal { position: fixed; inset: 0; background: rgba(0,0,0,0.9); display: none; align-items: center; justify-content: center; z-index: 100; }
        .modal-content { background: #1a1b26; padding: 40px; border-radius: 20px; max-width: 600px; width: 90%; max-height: 80vh; overflow-y: auto; border: 1px solid #333; }
    </style>
</head>
<body>

    <div class="orb-container">
        <div id="orb" class="orb"></div>
    </div>

    <div class="interface">
        <h1 id="mainTitle">Evalis AI</h1>
        <div id="status" class="status-pill">Waiting for Resume</div>
        
        <div id="setupPanel">
            <label class="file-trigger">
                <span id="fileLabel">ðŸ“‚ Upload PDF Resume</span>
                <input type="file" id="cvInput" accept=".pdf" style="display:none;">
            </label>
            <button id="startBtn" class="btn btn-primary" style="display:none; width:100%;">Start Interview</button>
        </div>

        <div id="activePanel" style="display:none;">
            <div id="timer" class="timer">15:00</div>
            <button id="endBtn" class="btn" style="background: #ef444420; color: #ef4444; border: 1px solid #ef444450;">End Session</button>
        </div>
    </div>

    <div id="reportModal" class="modal">
        <div class="modal-content">
            <h2 style="color:var(--primary); margin-top:0;">Evaluation Report</h2>
            <div id="reportText" style="line-height: 1.6; color: #cbd5e1; white-space: pre-wrap;"></div>
            <button onclick="location.reload()" class="btn btn-primary" style="width:100%; margin-top:20px;">Start New Session</button>
        </div>
    </div>

<script>
    const API_URL = "https://evalis-ai.simpaticohrconsultancy.workers.dev"; // Replace with your Worker URL
    pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js";

    let state = {
        cvText: "",
        history: [],
        isListening: false,
        timeLeft: 900
    };

    let audioCtx, analyser, dataArray;
    let recognition;
    let synth = window.speechSynthesis;
    let voices = [];

    // --- AUDIO & VISUALIZER ---
    async function initAudio() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioCtx.createAnalyser();
            analyser.fftSize = 64;
            const source = audioCtx.createMediaStreamSource(stream);
            source.connect(analyser);
            dataArray = new Uint8Array(analyser.frequencyBinCount);
            visualize();
            return true;
        } catch (e) {
            alert("Microphone access is required.");
            return false;
        }
    }

    function visualize() {
        requestAnimationFrame(visualize);
        const orb = document.getElementById('orb');
        
        // Priority: Speaking Animation > Thinking Animation > Listening Visualization
        if (orb.classList.contains('speaking')) return; 
        if (orb.classList.contains('thinking')) {
            orb.style.transform = 'scale(1)'; // Reset for CSS animation
            return;
        }

        if (state.isListening && analyser) {
            analyser.getByteFrequencyData(dataArray);
            const avg = dataArray.reduce((a, b) => a + b) / dataArray.length;
            const scale = 1 + (avg / 255) * 0.5;
            orb.style.transform = `scale(${scale})`;
        } else {
            orb.style.transform = `scale(1)`;
        }
    }

    // --- SPEECH RECOGNITION ---
    function setupRecognition() {
        const R = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!R) return alert("Browser not supported (Use Chrome/Edge).");
        
        recognition = new R();
        recognition.continuous = false;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
            state.isListening = true;
            updateStatus('listening');
        };

        recognition.onend = () => {
            state.isListening = false;
            // Only restart if we aren't processing or speaking
            const orb = document.getElementById('orb');
            if (!orb.classList.contains('thinking') && !orb.classList.contains('speaking') && document.getElementById('activePanel').style.display !== 'none') {
                recognition.start();
            }
        };

        recognition.onresult = async (e) => {
            const text = e.results[0][0].transcript;
            if(!text.trim()) return;
            
            updateStatus('thinking');
            recognition.stop(); // Stop mic while thinking
            await handleTurn(text);
        };
    }

    // --- SPEECH SYNTHESIS ---
    function loadVoices() {
        voices = synth.getVoices();
    }
    loadVoices();
    if (speechSynthesis.onvoiceschanged !== undefined) {
        speechSynthesis.onvoiceschanged = loadVoices;
    }

    function speak(text) {
        updateStatus('speaking');
        const u = new SpeechSynthesisUtterance(text);
        
        // Pick a better voice
        const preferred = voices.find(v => v.name.includes("Google US English") || v.name.includes("Natural"));
        if (preferred) u.voice = preferred;
        
        u.onend = () => {
            updateStatus('listening');
            recognition.start();
        };
        synth.speak(u);
    }

    // --- LOGIC ---
    function updateStatus(mode) {
        const orb = document.getElementById('orb');
        const status = document.getElementById('status');
        
        orb.className = 'orb'; // Reset
        
        if(mode === 'listening') {
            orb.classList.add('listening');
            status.textContent = "Listening...";
        } else if (mode === 'thinking') {
            orb.classList.add('thinking');
            status.textContent = "Analyzing...";
        } else if (mode === 'speaking') {
            orb.classList.add('speaking');
            status.textContent = "AI Speaking...";
        }
    }

    async function handleTurn(userText) {
        const fd = new FormData();
        fd.append("mode", "interview");
        fd.append("history", JSON.stringify(state.history));
        fd.append("cvText", state.cvText);
        fd.append("text", userText);

        try {
            const res = await fetch(API_URL, { method: "POST", body: fd });
            const data = await res.json();
            state.history = data.history;
            speak(data.reply);
        } catch (e) {
            console.error(e);
            speak("I lost connection. Please say that again.");
        }
    }

    // --- EVENT HANDLERS ---
    document.getElementById('cvInput').onchange = async (e) => {
        const file = e.target.files[0];
        if(!file) return;
        
        document.getElementById('fileLabel').textContent = "Processing...";
        const buffer = await file.arrayBuffer();
        const pdf = await pdfjsLib.getDocument(buffer).promise;
        let txt = "";
        for(let i=1; i<=pdf.numPages; i++) {
            const page = await pdf.getPage(i);
            const content = await page.getTextContent();
            txt += content.items.map(item => item.str).join(" ");
        }
        state.cvText = txt;
        document.getElementById('fileLabel').textContent = "Resume Ready âœ“";
        document.getElementById('startBtn').style.display = "block";
    };

    document.getElementById('startBtn').onclick = async () => {
        if (!await initAudio()) return;
        setupRecognition();
        
        document.getElementById('setupPanel').style.display = 'none';
        document.getElementById('activePanel').style.display = 'block';
        
        // Start Timer
        const tInterval = setInterval(() => {
            state.timeLeft--;
            const m = Math.floor(state.timeLeft / 60);
            const s = state.timeLeft % 60;
            document.getElementById('timer').textContent = `${m}:${s<10?'0'+s:s}`;
            if(state.timeLeft <= 0) finish();
        }, 1000);

        // Start Interaction
        updateStatus('thinking');
        const fd = new FormData();
        fd.append("mode", "start");
        fd.append("cvText", state.cvText);
        
        const res = await fetch(API_URL, { method: "POST", body: fd });
        const data = await res.json();
        state.history = data.history;
        speak(data.reply);
    };

    async function finish() {
        recognition.stop();
        synth.cancel();
        document.getElementById('activePanel').style.display = 'none';
        updateStatus('thinking');
        document.getElementById('status').textContent = "Generating Report...";

        const fd = new FormData();
        fd.append("mode", "score");
        fd.append("cvText", state.cvText); // Crucial: Send CV for final grading
        fd.append("history", JSON.stringify(state.history));

        const res = await fetch(API_URL, { method: "POST", body: fd });
        const data = await res.json();
        
        document.getElementById('reportText').textContent = data.reply;
        document.getElementById('reportModal').style.display = 'flex';
    }

    document.getElementById('endBtn').onclick = finish;
</script>
</body>
</html>
