<script>
/* =====================================================
   PLATFORM DETECTION
===================================================== */
const isAndroid = typeof Android !== "undefined";
const hasWebSpeech = 'webkitSpeechRecognition' in window;

/* =====================================================
   CONFIG
===================================================== */
const API_URL = "https://evalis-ai.simpaticohrconsultancy.workers.dev";

/* =====================================================
   APP STATE
===================================================== */
const State = {
  IDLE: "idle",
  LISTENING: "listening",
  THINKING: "thinking",
  SPEAKING: "speaking"
};

let app = {
  state: State.IDLE,
  history: [],
  cvText: "",
  active: false
};

/* =====================================================
   UI STATE DRIVER
===================================================== */
function setMode(mode) {
  app.state = mode;
  document.body.className = `mode-${mode}`;

  const map = {
    idle: "Ready",
    listening: "Listening…",
    thinking: "Analyzing…",
    speaking: "Interviewer Speaking"
  };
  document.getElementById("status").textContent = map[mode];
}

/* =====================================================
   SPEECH OUTPUT (AI → USER)
===================================================== */
const synth = window.speechSynthesis;

function speakAI(text) {
  setMode(State.SPEAKING);

  if (isAndroid) {
    Android.speak(text); // must call window.onFinishedSpeaking()
    return;
  }

  synth.cancel();
  const u = new SpeechSynthesisUtterance(text);
  u.rate = 1.05;
  u.onend = () => onSpeechEnd();
  synth.speak(u);
}

function onSpeechEnd() {
  if (!app.active) return;
  startListening();
}

/* =====================================================
   SPEECH INPUT (USER → AI)
===================================================== */
let recognition = null;

if (!isAndroid && hasWebSpeech) {
  recognition = new webkitSpeechRecognition();
  recognition.lang = "en-US";
  recognition.continuous = false;
  recognition.interimResults = false;

  recognition.onresult = (e) => {
    const text = e.results[0][0].transcript;
    onUserSpeech(text);
  };

  recognition.onerror = () => {
    if (app.active) {
      setTimeout(() => recognition.start(), 500);
    }
  };

  recognition.onend = () => {
    // Prevent dead state
    if (app.active && app.state === State.LISTENING) {
      setTimeout(() => recognition.start(), 300);
    }
  };
}

function startListening() {
  if (!app.active) return;

  setMode(State.LISTENING);

  if (isAndroid) {
    Android.startInterview(); // native mic → window.onRecognitionResult
  } else if (recognition) {
    try { recognition.start(); } catch {}
  }
}

function stopListening() {
  if (!isAndroid && recognition) {
    try { recognition.stop(); } catch {}
  }
}

/* =====================================================
   USER SPOKE → SEND TO AI
===================================================== */
function onUserSpeech(text) {
  if (!text || !text.trim()) {
    startListening();
    return;
  }

  stopListening();
  app.history.push({ role: "user", content: text });
  sendToAI("interview", text);
}

/* =====================================================
   AI REQUEST PIPELINE
===================================================== */
async function sendToAI(mode, text = "") {
  setMode(State.THINKING);

  const fd = new FormData();
  fd.append("mode", mode);
  fd.append("cvText", app.cvText);
  fd.append("history", JSON.stringify(app.history));
  if (text) fd.append("text", text);

  try {
    const res = await fetch(API_URL, { method: "POST", body: fd });
    const data = await res.json();

    if (data.reply) {
      app.history.push({ role: "assistant", content: data.reply });
      speakAI(data.reply);
    } else {
      speakAI("Let’s continue. Please explain further.");
    }
  } catch (e) {
    speakAI("Connection issue. Please continue.");
  }
}

/* =====================================================
   SESSION CONTROL
===================================================== */
async function startSession(file) {
  app.active = true;
  setMode(State.THINKING);

  // Parse PDF
  const buf = await file.arrayBuffer();
  const pdf = await pdfjsLib.getDocument(buf).promise;
  for (let i = 1; i <= pdf.numPages; i++) {
    const page = await pdf.getPage(i);
    const text = await page.getTextContent();
    app.cvText += text.items.map(s => s.str).join(" ");
  }

  sendToAI("start");
}

async function endSession() {
  app.active = false;
  stopListening();
  setMode(State.THINKING);

  const fd = new FormData();
  fd.append("mode", "score");
  fd.append("history", JSON.stringify(app.history));

  const res = await fetch(API_URL, { method: "POST", body: fd });
  const report = await res.json();
  showReport(report);
}

/* =====================================================
   ANDROID BRIDGE (MUST BE GLOBAL)
===================================================== */
window.onRecognitionResult = (text) => {
  onUserSpeech(text);
};

window.onFinishedSpeaking = () => {
  onSpeechEnd();
};

/* =====================================================
   WAKE LOCK (ANTI-SLEEP)
===================================================== */
let wakeLock = null;
async function keepAwake() {
  try { wakeLock = await navigator.wakeLock.request("screen"); } catch {}
}
document.addEventListener("visibilitychange", () => {
  if (document.visibilityState === "visible") keepAwake();
});
keepAwake();

/* =====================================================
   UI BINDINGS
===================================================== */
document.getElementById("cvInput").onchange = e =>
  startSession(e.target.files[0]);

document.getElementById("endBtn").onclick = endSession;
</script>
