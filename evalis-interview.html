<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>Evalis AI – Interview</title>

<script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.standard.js"></script>

<style>
body {
  margin: 0;
  background: #0f172a;
  font-family: system-ui;
  color: #e5e7eb;
  display: flex;
  justify-content: center;
  align-items: center;
  height: 100vh;
}

.card {
  background: #020617;
  padding: 28px;
  width: 92%;
  max-width: 420px;
  border-radius: 16px;
  box-shadow: 0 0 40px rgba(0,0,0,.6);
  text-align: center;
}

h3 { margin: 0 0 8px; }

#question {
  font-size: 18px;
  margin: 18px 0;
  min-height: 70px;
}

#status {
  font-size: 14px;
  color: #22c55e;
  min-height: 20px;
}

button {
  width: 100%;
  margin-top: 18px;
  padding: 14px;
  border-radius: 12px;
  border: none;
  background: #2563eb;
  color: white;
  font-size: 16px;
}
</style>
</head>

<body>
<div class="card">
  <h3>Evalis AI Interview</h3>
  <div id="question">Ready to begin</div>
  <div id="status">Tap start</div>
  <button id="startBtn">Start Interview</button>
</div>

<script>
const WORKER_URL = "https://evalis-ai.simpaticohrconsultancy.workers.dev";

let isAISpeaking = false;
let interviewStarted = false;
let recognitionActive = false;

const synth = window.speechSynthesis;
const statusEl = document.getElementById("status");
const questionEl = document.getElementById("question");

function setStatus(t) { statusEl.innerText = t; }

/* ---------- SPEAK ---------- */
function speak(text) {
  synth.cancel();
  const u = new SpeechSynthesisUtterance(text);
  u.lang = "en-US";
  u.rate = 0.95;

  u.onstart = () => {
    isAISpeaking = true;
    setStatus("AI speaking…");
  };

  u.onend = () => {
    isAISpeaking = false;
    setStatus("Listening…");
  };

  questionEl.innerText = text;
  synth.speak(u);
}

/* ---------- SPEECH RECOGNITION ---------- */
const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.lang = "en-US";
recognition.interimResults = false;
recognition.continuous = false;

recognition.onstart = () => recognitionActive = true;
recognition.onend = () => recognitionActive = false;

recognition.onresult = async (e) => {
  const answer = e.results[0][0].transcript;
  setStatus("Thinking…");

  try {
    const res = await fetch(WORKER_URL, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ text: answer })
    });
    const data = await res.json();
    speak(data.reply || "Can you explain that further?");
  } catch {
    speak("Please repeat that.");
  }
};

/* ---------- VAD ---------- */
let myvad;

async function initVAD() {
  myvad = await vad.MicVAD.new({
    onSpeechStart: () => {
      if (isAISpeaking) synth.cancel();
    },
    onSpeechEnd: () => {
      if (!recognitionActive && !isAISpeaking) {
        recognition.start();
      }
    }
  });
  myvad.start();
}

/* ---------- START ---------- */
document.getElementById("startBtn").onclick = async () => {
  if (interviewStarted) return;
  interviewStarted = true;
  document.getElementById("startBtn").style.display = "none";

  try {
    setStatus("Requesting microphone…");
    await navigator.mediaDevices.getUserMedia({ audio: true });
    await initVAD();
    speak("Tell me about yourself.");
  } catch (e) {
    setStatus("Microphone permission required");
  }
};
</script>
</body>
</html>
