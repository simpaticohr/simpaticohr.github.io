<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>Evalis AI – Interview</title>

<!-- VAD (Voice Activity Detection) -->
<script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.standard.js"></script>

<style>
body {
  margin: 0;
  background: #0f172a;
  font-family: system-ui;
  color: #e5e7eb;
  display: flex;
  justify-content: center;
  align-items: center;
  height: 100vh;
}

.card {
  background: #020617;
  padding: 28px;
  width: 92%;
  max-width: 420px;
  border-radius: 16px;
  box-shadow: 0 0 40px rgba(0,0,0,.6);
  text-align: center;
}

h3 { margin: 0 0 8px; }

#question {
  font-size: 18px;
  margin: 18px 0;
  min-height: 60px;
}

#status {
  font-size: 14px;
  color: #22c55e;
  min-height: 20px;
}

button {
  width: 100%;
  margin-top: 18px;
  padding: 14px;
  border-radius: 12px;
  border: none;
  background: #2563eb;
  color: white;
  font-size: 16px;
}
</style>
</head>

<body>
<div class="card">
  <h3>Evalis AI Interview</h3>
  <div id="question">Ready to begin</div>
  <div id="status">Tap start</div>
  <button id="startBtn">Start Interview</button>
</div>

<script>
/* ================= CONFIG ================= */
const WORKER_URL = "https://evalis-ai.simpaticohrconsultancy.workers.dev";

/* ================= STATE ================= */
let isAISpeaking = false;
let interviewStarted = false;

/* ================= SPEECH SYNTH ================= */
const synth = window.speechSynthesis;

function speak(text) {
  synth.cancel();
  const utter = new SpeechSynthesisUtterance(text);
  utter.lang = "en-US";
  utter.rate = 0.95;

  utter.onstart = () => {
    isAISpeaking = true;
    setStatus("AI speaking…");
  };

  utter.onend = () => {
    isAISpeaking = false;
    setStatus("Listening…");
  };

  document.getElementById("question").innerText = text;
  synth.speak(utter);
}

/* ================= UI ================= */
function setStatus(text) {
  document.getElementById("status").innerText = text;
}

/* ================= SPEECH RECOGNITION ================= */
const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.lang = "en-US";
recognition.interimResults = false;
recognition.continuous = false;

recognition.onresult = async (event) => {
  const answer = event.results[0][0].transcript;
  setStatus("Thinking…");

  try {
    const res = await fetch(WORKER_URL, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ text: answer })
    });

    const data = await res.json();
    speak(data.reply || "Can you explain that further?");
  } catch (e) {
    speak("Please repeat that.");
  }
};

/* ================= VAD ================= */
let myvad;

async function initVAD() {
  myvad = await vad.MicVAD.new({
    onSpeechStart: () => {
      if (isAISpeaking) {
        synth.cancel(); // interrupt AI
        isAISpeaking = false;
      }
    },
    onSpeechEnd: () => {
      recognition.start(); // user finished → STT
    }
  });
  myvad.start();
}

/* ================= START ================= */
document.getElementById("startBtn").onclick = async () => {
  if (interviewStarted) return;
  interviewStarted = true;
  document.getElementById("startBtn").style.display = "none";

  try {
    setStatus("Initializing microphone…");
    await initVAD();
    speak("Tell me about yourself.");
  } catch {
    setStatus("Microphone permission required");
  }
};
</script>
</body>
</html>
