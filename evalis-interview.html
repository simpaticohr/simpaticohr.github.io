<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evalis AI - Elite Live Interview</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <style>
        .back-btn {
    position: fixed;
    top: 18px;
    left: 18px;
    background: rgba(255,255,255,0.08);
    color: #fff;
    border: 1px solid rgba(255,255,255,0.15);
    backdrop-filter: blur(10px);
    padding: 10px 16px;
    border-radius: 12px;
    font-weight: 600;
    font-size: 14px;
    cursor: pointer;
    z-index: 999;
}

.back-btn:hover {
    background: rgba(255,255,255,0.15);
}
        :root { --primary: #6366f1; --bg: #020617; }
        body { margin: 0; background: var(--bg); color: white; font-family: system-ui, sans-serif; display: flex; align-items: center; justify-content: center; min-height: 100vh; overflow-x: hidden; }
        .card { width: 90%; max-width: 440px; background: rgba(255,255,255,0.05); backdrop-filter: blur(15px); border-radius: 24px; padding: 30px; text-align: center; border: 1px solid rgba(255,255,255,0.1); box-shadow: 0 25px 50px rgba(0,0,0,0.5); }
        .wave-box { height: 80px; margin: 15px 0; position: relative; }
        canvas { width: 100%; height: 60px; filter: drop-shadow(0 0 5px var(--primary)); }
        .listening-indicator { position: absolute; top: 0; right: 0; width: 12px; height: 12px; background: #10b981; border-radius: 50%; opacity: 0; transition: opacity 0.3s; }
        .listening-indicator.active { opacity: 1; animation: pulse 1.5s infinite; }
        @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.4; } }
        #timer { font-size: 24px; font-weight: 700; color: var(--primary); margin-bottom: 5px; font-variant-numeric: tabular-nums; }
        #status { font-size: 12px; color: #94a3b8; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 20px; }
        .btn { background: var(--primary); color: white; border: none; padding: 16px; border-radius: 14px; width: 100%; font-weight: bold; cursor: pointer; font-size: 16px; transition: 0.3s; }
        .btn:disabled { opacity: 0.3; cursor: not-allowed; }
        .btn:hover:not(:disabled) { transform: translateY(-2px); box-shadow: 0 10px 20px rgba(99,102,241,0.3); }
        #log { margin-top: 20px; max-height: 200px; overflow-y: auto; text-align: left; font-size: 14px; border-top: 1px solid #334155; padding-top: 15px; }
        .log-entry { margin-bottom: 10px; padding: 8px; border-radius: 8px; background: rgba(255,255,255,0.03); }
        .user-msg { color: #67e8f9; }
        .ai-msg { color: #a5b4fc; }
        .report-box { background: rgba(99, 102, 241, 0.1); padding: 20px; border-radius: 15px; text-align: left; line-height: 1.6; border: 1px solid var(--primary); }
        .error-msg { color: #ef4444; font-size: 12px; margin-top: 10px; }
    </style>
</head>
<body>

<div class="card">
    <div id="timer">15:00</div>
    <div id="status">Upload Resume to Start</div>
    <div class="wave-box">
        <canvas id="canvas"></canvas>
        <div class="listening-indicator" id="listeningIndicator"></div>
    </div>
    
    <input type="file" id="cvInput" accept=".pdf" style="margin-bottom:15px; font-size: 12px;">
    <button id="startBtn" class="btn" disabled>Start Interview</button>
    <button id="endBtn" class="btn" style="display:none; background:#ef4444">End Interview</button>
    
    <div id="log"></div>
    <div id="errorMsg" class="error-msg"></div>
</div>

<script>
const API = "https://evalis-ai.simpaticohrconsultancy.workers.dev";
const pdfjsLib = window['pdfjs-dist/build/pdf'];
pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js";

let cvText = "", history = [], interviewActive = false, isAISpeaking = false, recognition;
let audioCtx, analyser, dataArray, canvasCtx, timerInterval, timeLeft = 15 * 60;
let silenceTimeout, userSpeechBuffer = "", lastTranscriptTime = 0;
let isProcessing = false, reconnectAttempts = 0;

/* 1. IMPROVED PDF LOADING */
document.getElementById("cvInput").onchange = async e => {
    try {
        showError("");
        const file = e.target.files[0];
        if (!file) return;
        
        document.getElementById("status").textContent = "Loading Resume...";
        const pdf = await pdfjsLib.getDocument(await file.arrayBuffer()).promise;
        let text = "";
        
        for (let i = 1; i <= pdf.numPages; i++) {
            const page = await pdf.getPage(i);
            const content = await page.getTextContent();
            text += content.items.map(s => s.str).join(" ") + " ";
        }
        
        cvText = text.trim();
        
        cvText = text.trim();

// Android/WebView fallback
if (cvText.length < 30) {
    cvText = "[Resume uploaded ‚Äì text extraction limited]";
}

document.getElementById("startBtn").disabled = false;
document.getElementById("status").textContent = "Resume Uploaded ‚úì";
        
        document.getElementById("startBtn").disabled = false;
        document.getElementById("status").textContent = "Resume Ready ‚úì";
    } catch (err) {
        showError("Failed to load PDF. Please try again.");
        console.error(err);
    }
};

/* 2. ENHANCED WAVEFORM & TIMER */
function initWave(stream) {
    audioCtx = new AudioContext();
    analyser = audioCtx.createAnalyser();
    analyser.fftSize = 256;
    analyser.smoothingTimeConstant = 0.8;
    audioCtx.createMediaStreamSource(stream).connect(analyser);
    dataArray = new Uint8Array(analyser.frequencyBinCount);
    
    const canvas = document.getElementById("canvas");
    canvas.width = 400;
    canvas.height = 60;
    canvasCtx = canvas.getContext("2d");
    
    const draw = () => {
        if (!interviewActive) return;
        requestAnimationFrame(draw);
        analyser.getByteFrequencyData(dataArray);
        canvasCtx.clearRect(0, 0, 400, 60);
        canvasCtx.fillStyle = "#6366f1";
        
        for (let i = 0; i < 80; i++) {
            let h = dataArray[i] / 2.5;
            canvasCtx.fillRect(i * 5, 30 - h/2, 3, Math.max(h, 2));
        }
    };
    draw();
}

function startTimer() {
    timerInterval = setInterval(() => {
        timeLeft--;
        let min = Math.floor(timeLeft / 60), sec = timeLeft % 60;
        document.getElementById("timer").textContent = `${min}:${sec < 10 ? '0' : ''}${sec}`;
        if (timeLeft <= 0) finishInterview();
    }, 1000);
}

/* 3. OPTIMIZED SPEECH RECOGNITION */
function speak(text) {
    return new Promise((resolve) => {
        window.speechSynthesis.cancel();
        const u = new SpeechSynthesisUtterance(text);
        u.rate = 0.95;
        u.pitch = 1.0;
        
        u.onstart = () => { 
            isAISpeaking = true; 
            if(recognition) recognition.stop();
            updateListeningIndicator(false);
            document.getElementById("status").textContent = "AI Speaking...";
        };
        
        u.onend = () => { 
            isAISpeaking = false;
            if(interviewActive) {
                setTimeout(() => {
                    if (!isProcessing && interviewActive) {
                        recognition.start();
                        updateListeningIndicator(true);
                        document.getElementById("status").textContent = "Listening...";
                    }
                }, 500);
            }
            resolve();
        };
        
        u.onerror = () => {
            isAISpeaking = false;
            resolve();
        };
        
        window.speechSynthesis.speak(u);
        addLogEntry("AI", text);
    });
}

function initRecognition() {
    const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SR) {
        showError("Speech recognition not supported in this browser.");
        return;
    }
    
    recognition = new SR();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.maxAlternatives = 1;
    recognition.lang = 'en-US';
    
    recognition.onstart = () => {
        updateListeningIndicator(true);
        document.getElementById("status").textContent = "Listening...";
        reconnectAttempts = 0;
    };
    
    recognition.onresult = async e => {
        const current = e.results[e.results.length - 1];
        const transcript = current[0].transcript.trim();
        
        if (current.isFinal && transcript.length > 0) {
            clearTimeout(silenceTimeout);
            
            if (isAISpeaking || isProcessing) return;
            
            userSpeechBuffer = transcript;
            lastTranscriptTime = Date.now();
            
            // Wait for natural pause before processing
            silenceTimeout = setTimeout(async () => {
                if (userSpeechBuffer && !isProcessing && !isAISpeaking) {
                    await processUserInput(userSpeechBuffer);
                    userSpeechBuffer = "";
                }
            }, 1200);
        }
    };
    
    recognition.onerror = e => {
        console.error("Recognition error:", e.error);
        if (e.error === 'no-speech') {
            // Ignore no-speech errors
            return;
        }
        
        if (e.error === 'aborted' && !interviewActive) {
            return;
        }
        
        if (interviewActive && !isAISpeaking && reconnectAttempts < 3) {
            setTimeout(() => {
                if (interviewActive && !isAISpeaking) {
                    recognition.start();
                    reconnectAttempts++;
                }
            }, 1000);
        }
    };
    
    recognition.onend = () => {
        updateListeningIndicator(false);
        if (interviewActive && !isAISpeaking && !isProcessing) {
            setTimeout(() => {
                if (interviewActive && !isAISpeaking) {
                    recognition.start();
                }
            }, 300);
        }
    };
}

async function processUserInput(transcript) {
    if (isProcessing || !transcript) return;
    
    isProcessing = true;
    recognition.stop();
    updateListeningIndicator(false);
    document.getElementById("status").textContent = "Thinking...";
    
    addLogEntry("You", transcript);
    
    try {
        const fd = new FormData();
        fd.append("mode", "interview");
        fd.append("history", JSON.stringify(history));
        fd.append("text", transcript);
        fd.append("cvText", cvText);

        const res = await fetch(API, { 
            method: "POST", 
            body: fd,
            signal: AbortSignal.timeout(30000)
        });
        
        if (!res.ok) throw new Error(`Server error: ${res.status}`);
        
        const data = await res.json();
        history = data.history || history;
        
        if (data.reply) {
            await speak(data.reply);
        }
    } catch (err) {
        console.error("API Error:", err);
        showError("Connection issue. Please try again.");
        if (interviewActive) {
            setTimeout(() => {
                recognition.start();
                isProcessing = false;
            }, 2000);
        }
    } finally {
        isProcessing = false;
    }
}

/* 4. IMPROVED SCORING */
async function finishInterview() {
    interviewActive = false;
    clearInterval(timerInterval);
    clearTimeout(silenceTimeout);
    if(recognition) recognition.stop();
    window.speechSynthesis.cancel();
    updateListeningIndicator(false);
    
    document.getElementById("status").textContent = "Generating Score Report...";
    
    try {
        const fd = new FormData();
        fd.append("mode", "score");
        fd.append("history", JSON.stringify(history));
        fd.append("cvText", cvText);
        fd.append("duration", String(15 * 60 - timeLeft));

        const res = await fetch(API, { 
            method: "POST", 
            body: fd,
            signal: AbortSignal.timeout(45000)
        });
        
        if (!res.ok) throw new Error(`Server error: ${res.status}`);
        
        const data = await res.json();
        
        document.body.innerHTML = `
            <div class="card" style="max-width:700px">
                <h2 style="color:var(--primary); margin-bottom: 20px;">Final Score Report</h2>
                <div class="report-box" style="white-space:pre-wrap; font-size: 14px;">${data.reply || "Report generation failed."}</div>
                <button onclick="location.reload()" class="btn" style="margin-top:20px">New Interview Session</button>
            </div>`;
    } catch (err) {
        console.error("Scoring error:", err);
        showError("Failed to generate report. Please try again.");
    }
}

/* 5. START SESSION */
document.getElementById("startBtn").onclick = async () => {
    try {
        showError("");

        // üé§ MUST be first (Android WebView rule)
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        interviewActive = true;

        initWave(stream);
        initRecognition();
        startTimer();

        document.getElementById("cvInput").style.display = "none";
        document.getElementById("startBtn").style.display = "none";
        document.getElementById("endBtn").style.display = "block";
        document.getElementById("status").textContent = "Initializing...";

        // üîó Backend start (NO mic interaction here)
        const fd = new FormData();
        fd.append("mode", "start");
        fd.append("cvText", cvText);

        const res = await fetch(API, {
            method: "POST",
            body: fd,
            signal: AbortSignal.timeout(15000)
        });

        if (!res.ok) throw new Error(`Server error: ${res.status}`);

        const data = await res.json();
        history = data.history || [];

        // üîä AI speaks AFTER mic + recognition ready
        if (data.reply) {
            await speak(data.reply);
        }

    } catch (err) {
        console.error("Start error:", err);
        showError("Microphone blocked. Close app and try again.");
        interviewActive = false;
        clearInterval(timerInterval);
    }
};
        showError("");
        
        // Test audio permissions
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // Unlock audio context
        window.speechSynthesis.speak(new SpeechSynthesisUtterance(""));
        
        interviewActive = true;
        initWave(stream);
        initRecognition();
        startTimer();

        document.getElementById("cvInput").style.display = "none";
        document.getElementById("startBtn").style.display = "none";
        document.getElementById("endBtn").style.display = "block";
        document.getElementById("status").textContent = "Initializing...";

        const fd = new FormData();
        fd.append("mode", "start");
        fd.append("cvText", cvText);
        
        const res = await fetch(API, { 
            method: "POST", 
            body: fd,
            signal: AbortSignal.timeout(15000)
        });
        
        if (!res.ok) throw new Error(`Server error: ${res.status}`);
        
        const data = await res.json();
        history = data.history || [];
        
        if (data.reply) {
            await speak(data.reply);
        }
    } catch (err) {
        console.error("Start error:", err);
        showError("Failed to start interview. Please check your microphone and try again.");
        interviewActive = false;
        clearInterval(timerInterval);
    }
};

document.getElementById("endBtn").onclick = () => {
    if (confirm("Are you sure you want to end the interview?")) {
        finishInterview();
    }
};

/* UTILITY FUNCTIONS */
function updateListeningIndicator(active) {
    const indicator = document.getElementById("listeningIndicator");
    if (active) {
        indicator.classList.add("active");
    } else {
        indicator.classList.remove("active");
    }
}

function addLogEntry(speaker, text) {
    const entry = document.createElement("div");
    entry.className = `log-entry ${speaker === "AI" ? "ai-msg" : "user-msg"}`;
    entry.innerHTML = `<b>${speaker}:</b> ${text}`;
    document.getElementById("log").prepend(entry);
}

function showError(msg) {
    document.getElementById("errorMsg").textContent = msg;
}

// Prevent page unload during interview
window.addEventListener('beforeunload', (e) => {
    if (interviewActive) {
        e.preventDefault();
        e.returnValue = '';
    }
});
 document.getElementById("backBtn").onclick = () => {
    try {
        recognition?.stop();
        window.speechSynthesis.cancel();
        audioCtx?.close();
        interviewActive = false;
    } catch (e) {}

    window.location.href = "career-lab.html";
};

// Android back button support
history.pushState(null, "", location.href);
window.onpopstate = () => {
    document.getElementById("backBtn").click();
};   
</script>
</body>
    <!-- üîô Back to Career Lab -->
<button id="backBtn" class="back-btn">‚Üê Back</button>
</html>
