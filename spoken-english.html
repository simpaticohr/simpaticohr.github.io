<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Elite AI Tutor</title>
    <script src="https://cdn.tailwindcss.com"></script>
    // Add this at the very top of your <script>
let voices = [];
window.speechSynthesis.onvoiceschanged = () => {
  voices = window.speechSynthesis.getVoices();
};
    <style>
        body { background: #0f172a; color: white; font-family: sans-serif; }
        .orb {
            width: 150px; height: 150px;
            border-radius: 50%;
            background: radial-gradient(circle, #3b82f6, #1e40af);
            box-shadow: 0 0 50px #3b82f6;
            transition: all 0.2s ease;
        }
        .orb.listening { animation: pulse 1.5s infinite; background: radial-gradient(circle, #ef4444, #991b1b); box-shadow: 0 0 50px #ef4444; }
        .orb.speaking { animation: bounce 0.5s infinite alternate; background: radial-gradient(circle, #10b981, #047857); box-shadow: 0 0 50px #10b981; }
        @keyframes pulse { 0% { transform: scale(1); opacity: 1; } 50% { transform: scale(1.1); opacity: 0.8; } 100% { transform: scale(1); opacity: 1; } }
        @keyframes bounce { from { transform: scale(1); } to { transform: scale(1.05); } }
    </style>
</head>
<body class="h-screen flex flex-col items-center justify-center relative overflow-hidden">

    <div class="absolute top-10 text-center">
        <h1 class="text-3xl font-bold tracking-tight mb-2">English Tutor AI</h1>
        <p id="status" class="text-slate-400 text-sm">Tap the orb to start the session</p>
    </div>

    <div id="orb" class="orb flex items-center justify-center cursor-pointer mb-12">
        <svg id="icon" class="w-12 h-12 text-white/80" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"></path></svg>
    </div>

    <div class="w-full max-w-md px-6 space-y-4">
        <div id="userText" class="text-right text-blue-300 text-lg opacity-0 transition-opacity"></div>
        <div id="aiText" class="text-left text-green-300 text-lg opacity-0 transition-opacity"></div>
    </div>

    <script>
        // CONFIG
        const WORKER_URL = 'https://divine-meadow-322b.simpaticohrconsultancy.workers.dev'; 
        const SILENCE_THRESHOLD = 1500; // Time in ms to wait after silence to send
        
        // STATE
        let mediaRecorder;
        let audioChunks = [];
        let isSessionActive = false;
        let audioContext, analyser, microphone, silenceTimer;
        let conversationHistory = [];

        const orb = document.getElementById('orb');
        const status = document.getElementById('status');
        const userDisplay = document.getElementById('userText');
        const aiDisplay = document.getElementById('aiText');

        // 1. INITIALIZE & PERMISSIONS
        orb.addEventListener('click', async () => {
            if (!isSessionActive) {
                await startSession();
            } else {
                stopSession();
            }
        });

        async function startSession() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                isSessionActive = true;
                status.textContent = "Listening... (Auto-send on silence)";
                orb.classList.add('listening');
                
                // Initialize Silence Detection
                setupSilenceDetection(stream);
                // Initialize Recorder
                setupRecorder(stream);

            } catch (e) {
                alert("Microphone access denied or error: " + e.message);
            }
        }

        function stopSession() {
            isSessionActive = false;
            orb.classList.remove('listening', 'speaking');
            status.textContent = "Session Paused. Tap to resume.";
            if (mediaRecorder) mediaRecorder.stop();
            if (audioContext) audioContext.close();
            clearTimeout(silenceTimer);
        }

        // 2. RECORDER SETUP (With MIME Type Fix)
        function setupRecorder(stream) {
            const mimeTypes = ['audio/webm', 'audio/mp4', 'audio/ogg', 'audio/wav'];
            const mimeType = mimeTypes.find(type => MediaRecorder.isTypeSupported(type)) || '';
            
            mediaRecorder = new MediaRecorder(stream, { mimeType });
            
            mediaRecorder.ondataavailable = e => {
                if (e.data.size > 0) audioChunks.push(e.data);
            };

            mediaRecorder.onstop = async () => {
                if (!isSessionActive || audioChunks.length === 0) return;
                
                status.textContent = "Thinking...";
                orb.classList.remove('listening');
                
                const audioBlob = new Blob(audioChunks, { type: mimeType });
                audioChunks = []; // Reset for next turn
                
                await processAudio(audioBlob);
            };

            mediaRecorder.start();
        }

        // 3. SILENCE DETECTION (The "Hands-Free" Magic)
        function setupSilenceDetection(stream) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(stream);
            const scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1);

            analyser.fftSize = 512;
            microphone.connect(analyser);
            analyser.connect(scriptProcessor);
            scriptProcessor.connect(audioContext.destination);

            scriptProcessor.onaudioprocess = () => {
                if (!isSessionActive) return;

                const array = new Uint8Array(analyser.frequencyBinCount);
                analyser.getByteFrequencyData(array);
                
                // Calculate average volume
                let values = 0;
                for (let i = 0; i < array.length; i++) values += array[i];
                const average = values / array.length;

                // Simple Threshold Logic
                if (average > 10) { // User is talking
                    clearTimeout(silenceTimer);
                    orb.classList.add('listening'); // Pulse effect
                } else {
                    // Silence detected, start timer
                    if (mediaRecorder && mediaRecorder.state === 'recording') {
                         // Only set timer if we haven't already
                         if (!silenceTimer) {
                             silenceTimer = setTimeout(() => {
                                 console.log("Silence detected, sending...");
                                 mediaRecorder.stop(); // This triggers onstop -> processAudio
                                 silenceTimer = null;
                             }, SILENCE_THRESHOLD);
                         }
                    }
                }
            };
        }

        // 4. API & AI LOGIC
        async function processAudio(audioBlob) {
            // A. Transcribe
            const formData = new FormData();
            formData.append('audio', audioBlob);

            try {
                const transRes = await fetch(`${WORKER_URL}/api/transcribe`, { method: 'POST', body: formData });
                const transData = await transRes.json();
                
                if (!transData.success) throw new Error(transData.error || "Transcription failed");
                
                const userText = transData.transcription;
                if (!userText.trim()) {
                    // False alarm (noise), restart recorder
                    mediaRecorder.start();
                    return;
                }

                userDisplay.textContent = `"${userText}"`;
                userDisplay.classList.remove('opacity-0');

                // B. Get AI Reply
                const chatRes = await fetch(`${WORKER_URL}/api/chat`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ message: userText, history: conversationHistory })
                });
                const chatData = await chatRes.json();
                
                const aiResponse = chatData.response;
                conversationHistory.push({ role: 'user', content: userText }, { role: 'assistant', content: aiResponse });
                conversationHistory = conversationHistory.slice(-6); // Keep history small

                aiDisplay.textContent = aiResponse;
                aiDisplay.classList.remove('opacity-0');

                // C. Speak (TTS)
                speakResponse(aiResponse);

            } catch (error) {
                console.error(error);
                status.textContent = "Error: " + error.message;
                setTimeout(() => { if(isSessionActive) mediaRecorder.start(); }, 2000);
            }
        }

        // 5. TEXT TO SPEECH (Browser Native)
        function speakResponse(text) {
            orb.classList.add('speaking');
            status.textContent = "Speaking...";

            // Cancel any previous speech
            window.speechSynthesis.cancel();

            const utterance = new SpeechSynthesisUtterance(text);
            
            // Try to find a good English voice
            const voices = window.speechSynthesis.getVoices();
            const preferredVoice = voices.find(v => v.name.includes("Google US English")) || 
                                   voices.find(v => v.lang === 'en-US');
            if (preferredVoice) utterance.voice = preferredVoice;

            utterance.rate = 1.0;
            utterance.pitch = 1.0;

            utterance.onend = () => {
                orb.classList.remove('speaking');
                status.textContent = "Listening...";
                // IMPORTANT: Restart recording only after AI finishes speaking
                if (isSessionActive && mediaRecorder.state === 'inactive') {
                    mediaRecorder.start();
                }
            };

            window.speechSynthesis.speak(utterance);
        }
    </script>
</body>
</html>
